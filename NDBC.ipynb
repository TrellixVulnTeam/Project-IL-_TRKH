{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NDBC.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"f0ee95dff3324541acfec08c4c64cbd1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a0333af574dd4cd7bf825b0a0eaf3e0c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5662dc1d9c984afca3e585653e0b4c3b","IPY_MODEL_23ec2142dfdf450ab55858a050d23b94"]}},"a0333af574dd4cd7bf825b0a0eaf3e0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5662dc1d9c984afca3e585653e0b4c3b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6a3e5aa9eab94088b0fd68554e102985","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c6ce8ec050be4a989a1fbd1be71bd55d"}},"23ec2142dfdf450ab55858a050d23b94":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0e5add9b7c364317b036961d1bf52224","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 169009152/? [00:20&lt;00:00, 33416810.38it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_caec53d3649b4f6798c2c815b5a9bb12"}},"6a3e5aa9eab94088b0fd68554e102985":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c6ce8ec050be4a989a1fbd1be71bd55d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0e5add9b7c364317b036961d1bf52224":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"caec53d3649b4f6798c2c815b5a9bb12":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"GI5U_VjS82Oq","colab_type":"text"},"source":["**Import utilities**"]},{"cell_type":"code","metadata":{"id":"38UuA0Ubj-Ku","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1593129266600,"user_tz":-120,"elapsed":17580,"user":{"displayName":"Marco Ratta","photoUrl":"","userId":"08139811788458053031"}},"outputId":"a956f704-075d-4512-9b79-259484f03ea9"},"source":["import torch\n","import random\n","from scipy.stats import multivariate_normal\n","import os\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import numpy as np\n","from PIL import Image\n","import torch.optim as optim \n","import torchvision.datasets as dsets\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from torch.utils.data import Subset, DataLoader\n","import sys\n","import matplotlib.pyplot as plt\n","import matplotlib.gridspec as gridspec\n","from torch.backends import * \n","from sklearn import covariance\n","\n","DEVICE = torch.device('cuda:0')\n","\n","if not os.path.isdir('./Project-dir'):\n","  !git clone https://github.com/armando-larocca/Project-IL-\n","\n","if not os.path.isdir('content/cifar100.py'):\n","  !mv '/content/Project-IL-/cifar100.py' '/content'  \n","  !mv '/content/Project-IL-/utils.py' '/content'  \n","\n","if not os.path.isdir('content/cifarResnet.py'):\n","  !mv '/content/Project-IL-/cifarResnet.py' '/content'\n","\n","if not os.path.isdir('content/icarl_utils.py'):\n","  !mv '/content/Project-IL-/icarl_utils.py' '/content'  \n","\n","from cifarResnet import resnet32\n","from cifar100 import *\n","from icarl_utils import * \n","\n","# Hyper Parameters\n","random.seed(653)\n","num_epochs = 70\n","batch_size = 128\n","total_classes = 100\n","num_classes = 10\n","indici_classi = list(range(0,100))\n","random.shuffle(indici_classi)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'Project-IL-'...\n","remote: Enumerating objects: 47, done.\u001b[K\n","remote: Counting objects: 100% (47/47), done.\u001b[K\n","remote: Compressing objects: 100% (45/45), done.\u001b[K\n","remote: Total 47 (delta 14), reused 0 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (47/47), done.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1CdFb8elLEJG","colab_type":"text"},"source":["**Weights initialization**"]},{"cell_type":"code","metadata":{"id":"omjdWY74ddV9","colab_type":"code","colab":{}},"source":["def init_weights(m):\n","  if type(m) == nn.Conv2d:\n","    torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n","\n","  if type(m) == nn.Linear:\n","    torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bcsTwgvOLIrm","colab_type":"text"},"source":["**ICARL**"]},{"cell_type":"code","metadata":{"id":"9jyhGkpp8xjX","colab_type":"code","colab":{}},"source":["\n","class iCaRLNet(nn.Module):\n","    def __init__(self, feature_size, n_classes):\n","        \n","        #### Network architecture #### \n","        super(iCaRLNet, self).__init__()\n","        self.feature_extractor = resnet32()\n","        self.fc = nn.Linear(feature_size, n_classes, bias=True)\n","\n","        self.n_classes = n_classes\n","        self.n_known = 0\n","\n","        self.cls_loss = nn.BCEWithLogitsLoss()\n","\n","        self.exemplar_sets = []\n","        self.indexes = []\n","        self.ind = []\n","\n","        # Means of exemplars\n","        self.compute_means = True\n","        self.exemplar_means = []\n","        self.exemplar_variance = []\n","\n","        self.apply(init_weights)\n","\n","\n","\n","    def forward(self, x):\n","        x = self.feature_extractor(x)\n","        x = self.fc(x)\n","        return x\n","\n","\n","\n","    def increment_classes(self, n):\n","        ### INCREMENT THE NUMBER OF OUTPUTS  ###\n","        ### AND COPY THE WEIGHTS            ####\n","        torch.no_grad()\n","        self.train(False)\n","        in_features = self.fc.in_features\n","        out_features = self.fc.out_features\n","        weight = self.fc.weight.data\n","        bias = self.fc.bias.data\n","\n","        self.fc = nn.Linear(in_features, out_features+n, bias=True)\n","        self.fc.weight.data[:out_features] = weight\n","        self.fc.bias.data[:out_features] = bias\n","        self.n_classes += n\n","\n","\n","\n","\n","    def classify(self, x, transform):\n","        \"\"\"Classify images by neares-means-of-exemplars\n","        Args:\n","            x: input image batch\n","        Returns:\n","            preds: Tensor of size (batch_size,)\n","        \"\"\"\n","        torch.no_grad()\n","        self.train(False)\n","        batch_size = x.size(0)\n","\n","        if self.compute_means:\n","            print (\"Computing mean of exemplars...\")\n","            exemplar_means = []\n","            exemplar_variance = []\n","            for P_y in self.exemplar_sets:\n","                features = []\n","                # Extract feature for each exemplar in P_y\n","                for ex in P_y:\n","                    ex = Variable(ex, volatile=True).cuda()\n","                    feature = self.feature_extractor(ex.unsqueeze(0))\n","                    feature = feature.squeeze()\n","                    feature.data = feature.data / feature.data.norm() # Normalize\n","                    features.append(feature)\n","                features = torch.stack(features)\n","                mu_y = features.mean(0).squeeze()\n","                media = mu_y\n","                mu_y.data = mu_y.data / mu_y.data.norm() # Normalize\n","                exemplar_means.append(mu_y)\n","                features = features.cpu()\n","                cov, shrinkage = covariance.ledoit_wolf(np.array(features), assume_centered=True, block_size=1000)\n","                #cov, shrinkage = covariance.oas(np.array(features), assume_centered=True)\n","                #cov = np.cov(np.array(features).T)\n","                #cov += np.eye(64) * 1e-3\n","                exemplar_variance.append(cov)\n","            self.exemplar_means = exemplar_means\n","            self.exemplar_variance = exemplar_variance\n","            self.compute_means = False\n","            print (\"Done\")\n","\n","        exemplar_means = self.exemplar_means\n","        exemplar_variance = self.exemplar_variance\n","\n","        means = torch.stack(exemplar_means) # (n_classes, feature_size)\n","        means = torch.stack([means] * batch_size) # (batch_size, n_classes, feature_size)\n","        means = means.transpose(1, 2) # (batch_size, feature_size, n_classes)\n","\n","        labels = []\n","        feature = self.feature_extractor(x) # (batch_size, feature_size)\n","        for i in range(0,feature.size(0)): # Normalize\n","            feature.data[i] = feature.data[i] / feature.data[i].norm()\n","        #feature = feature.unsqueeze(2) # (batch_size, feature_size, 1)\n","        #feature = feature.expand_as(means) # (batch_size, feature_size, n_classes)\n","        \n","        for fea in feature:\n","         fea = fea.cpu()\n","         best_prob = 0\n","         for i in range (0, len(self.exemplar_means)):\n","            exemplar_means[i] = exemplar_means[i].cpu()\n","            Mean =np.array(exemplar_means[i]).T\n","            Cov = exemplar_variance[i]\n","            prob = multivariate_normal.pdf(fea, mean = Mean , cov=Cov, allow_singular=True)\n","            if prob > best_prob:\n","              best_prob = prob\n","              best_i = i\n","         labels.append(best_i)\n","\n","\n","\n","        '''dists = (feature - means).pow(2).sum(1).squeeze() #(batch_size, n_classes)\n","        _, preds = dists.min(1) \n","\n","        labels = []\n","        print(len(x))\n","        for i in range(0, len(x)):\n","         best_prob = 0\n","         feature = self.feature_extractor(x[i])\n","         feature.data = feature.data / feature.data.norm()\n","         for i in range (0, len(self.exemplar_means)):\n","           Mean =np.array(exemplar_means[i]).T\n","           Cov = exemplar_variance[i]\n","           prob = multivariate_normal.pdf(feature, mean = Mean , cov=Cov)\n","           if prob > best_prob:\n","             best_prob = prob\n","             labels.append(i)\n","          \n","\n","\n","        #return preds\n","        print(labels)'''\n","        return labels\n","        \n","\n","\n","    def construct_exemplar_set(self, images, indicis, transform):\n","  \n","        \"\"\"Construct an exemplar set for image set\n","        Args:\n","            images: np.array containing images of a class\n","        \"\"\"\n","        torch.no_grad()\n","        self.train(False)\n","        # Compute and cache features for each example\n","        features = []\n","        for img in images:\n","            x = Variable(img, volatile=True).cuda()\n","            feature = self.feature_extractor(x.unsqueeze(0)).data.cpu().numpy()\n","            feature = feature / np.linalg.norm(feature) # Normalize\n","            features.append(feature[0])\n","\n","        features = np.array(features)\n","        class_mean = np.mean(features, axis=0)\n","        class_mean = class_mean / np.linalg.norm(class_mean) # Normalize\n","\n","        exemplar_set = []\n","        exemplar_features = [] # list of Variables of shape (feature_size,)\n","        indici = []\n","        for k in range(0, len(images)):\n","            S = np.sum(exemplar_features, axis=0)\n","            phi = features\n","            mu = class_mean\n","            mu_p = 1.0/(k+1) * (phi + S)\n","            mu_p = mu_p / np.linalg.norm(mu_p)\n","            i = np.argmin(np.sqrt(np.sum((mu - mu_p) ** 2, axis=1)))\n","            exemplar_set.append(images[i])\n","            indici.append(indicis[i])\n","            exemplar_features.append(features[i])\n","          \n","            #print (\"Selected example\", i)\n","            #print (\"|exemplar_mean - class_mean|:\")\n","            #print (np.linalg.norm((np.mean(exemplar_features, axis=0) - class_mean)))\n","            features = np.delete(features, i, axis=0)\n","            indicis = np.delete(indicis, i, axis=0)\n","            \n","        #exemplar_set = torch.stack(exemplar_set)\n","\n","        ordine = list(range(0,len(indici)))\n","        random.shuffle(ordine)\n","        indici_rnd = []\n","        exemplars_rnd = []\n","        for i in range(0,len(ordine)):\n","          indici_rnd.append(indici[ordine[i]])\n","          exemplars_rnd.append(exemplar_set[ordine[i]])\n","\n","        self.exemplar_sets.append(exemplars_rnd)\n","        self.indexes.append(indici_rnd)\n","\n","        #self.exemplar_sets.append(exemplar_set)\n","        #self.indexes.append(indici)\n","        #random.shuffle(indici)\n","                \n","\n","\n","    def reduce_exemplar_sets(self, m):\n","        self.train(False)\n","        for y, P_y in enumerate(self.exemplar_sets):\n","            self.exemplar_sets[y] = P_y[:int(m)]\n","            uff = self.indexes[y] \n","            uff = uff[:int(m)]\n","            self.indexes[y] = uff\n","\n","\n","\n","    def combine_dataset_with_exemplars(self, dataset, superdataset):\n","        self.train(False)\n","        indici = []\n","        a = list(dataset)\n","        for item in a:\n","          indici.append(item[0])\n","        for j in self.indexes:\n","           indici = indici + j\n","        print(len(indici))\n","        dataset = Subset(superdataset, indici)\n","\n","        return dataset\n","            \n","        \n","\n","\n","    def update_representation(self, dataset, superdataset):\n","\n","        self.cuda()\n","        p = icarl.parameters()\n","        optimizer = optim.SGD(p, lr=2, weight_decay = 0.00001, momentum=0.9)\n","        #self.cuda()\n","        \n","        dataset = self.combine_dataset_with_exemplars(dataset, superdataset)         \n","        loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,shuffle=True, num_workers=4)\n","\n","        self.compute_means = True\n","        self.train(False)\n","        \n","        # Store network outputs with pre-update parameters\n","        self.train(False)\n","        q = torch.zeros(52000, self.n_classes).cuda()\n","        for  indices, images, labels in loader:\n","            images = Variable(images).cuda()\n","            indices = indices.cuda()\n","            g = F.sigmoid(self.forward(images))\n","            q[indices] = g.data         \n","        q = Variable(q).cuda()\n","\n","        \n","        # Increment number of weights in final fc layer\n","        classes = list(set(lb(dataset)))\n","        new_classes = [cls for cls in classes if cls > self.n_classes - 1]\n","\n","        if(len(new_classes) != 0):\n","          print(\"#####################\")\n","          self.increment_classes(len(new_classes))\n","          self.cuda()\n","          p = self.parameters() \n","          optimizer = optim.SGD(p, lr=2.0, weight_decay = 0.00001, momentum=0.9)\n","        #self.cuda()\n","        print (\"%d new classes\" % (len(new_classes)))\n","        \n","        tot_matr =[]\n","        tot_lab=[]\n","        \n","        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [49,63], gamma=0.2)\n","    \n","        for epoch in range(0,num_epochs):\n","            running_corrects = 0\n","            total = 0\n","            matr =[]\n","            lab =[]\n","             \n","            for i, (indices, images, labels) in enumerate(loader):\n","        \n","                images = Variable(images).cuda()\n","                labels = Variable(labels).cuda()\n","                indices = indices.cuda()\n","\n","                self.train(True)\n","                optimizer.zero_grad()\n","                g = self.forward(images)\n","                \n","                _, preds = torch.max(g, 1)\n","                running_corrects += torch.sum(preds == labels.data).data.item()\n","                total += labels.size(0)\n","\n","                matr.extend(preds)\n","                lab.extend(labels)\n","\n","                q_i = q[indices]\n","                Id = torch.eye(self.n_known+10)\n","                one_hot = Id[labels].cuda()\n","                one_hot[:,:self.n_known] = q_i[:,:self.n_known]\n","                loss = self.cls_loss(g, one_hot) \n","\n","                loss.backward()\n","                optimizer.step()\n","                torch.cuda.empty_cache()\n","\n","            self.train(False)\n","            tot_lab.append(lab)\n","            tot_matr.append(matr)\n","\n","            scheduler.step()\n","            accuracy = running_corrects / float(total)\n","          \n","            print(\"LR:\",scheduler.get_last_lr())\n","            print ('Epoch [%d/%d], Loss: %.4f, Acc: %.2f ' %(epoch+1, num_epochs, loss.data, accuracy,)) \n","\n","        return tot_lab, tot_matr      \n","                "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xCYSysgDOhXM","colab_type":"text"},"source":["**Dataset preparation**"]},{"cell_type":"code","metadata":{"id":"Ui_OaQofk04A","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":117,"referenced_widgets":["f0ee95dff3324541acfec08c4c64cbd1","a0333af574dd4cd7bf825b0a0eaf3e0c","5662dc1d9c984afca3e585653e0b4c3b","23ec2142dfdf450ab55858a050d23b94","6a3e5aa9eab94088b0fd68554e102985","c6ce8ec050be4a989a1fbd1be71bd55d","0e5add9b7c364317b036961d1bf52224","caec53d3649b4f6798c2c815b5a9bb12"]},"executionInfo":{"status":"ok","timestamp":1593129280116,"user_tz":-120,"elapsed":31040,"user":{"displayName":"Marco Ratta","photoUrl":"","userId":"08139811788458053031"}},"outputId":"1ebe8957-3d3d-47b7-f9e1-55e1ca7ab8df"},"source":["transform = transforms.Compose([\n","        transforms.RandomCrop(32, padding=4),\n","        transforms.RandomHorizontalFlip(0.5),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n","])\n","\n","transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n","])\n","\n","train_dataset = Cifar100(\".\\Data\", train=True, transform=transform)\n","test_dataset = Cifar100(\".\\Data\", train=False, transform=transform_test)\n","\n","train_dataset._shuffle_(indici_classi)\n","test_dataset._shuffle_(indici_classi)\n","\n","incr_train = train_dataset.__incremental_train_indexes__(1)\n","incr_val = test_dataset.__incremental_val_indexes__(0)\n","\n","decine_train = []\n","decine_val = []\n","\n","for i in range(0,10):\n","  val_dataset = Subset(test_dataset, incr_val[i])\n","  training_dataset = Subset(train_dataset, incr_train[i])\n","  decine_train.append(training_dataset)\n","  decine_val.append(val_dataset) \n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to .\\Data/cifar-100-python.tar.gz\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f0ee95dff3324541acfec08c4c64cbd1","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Extracting .\\Data/cifar-100-python.tar.gz to .\\Data\n","Using downloaded and verified file: .\\Data/cifar-100-python.tar.gz\n","Extracting .\\Data/cifar-100-python.tar.gz to .\\Data\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6R5he6AELS7j","colab_type":"text"},"source":["**Main**"]},{"cell_type":"code","metadata":{"id":"uzq10DaWMfDt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"outputId":"636a6041-6d5e-4ec8-ee26-f20d05d59440"},"source":["# Initialize CNN\n","K = 2000 # total number of exemplars\n","icarl = iCaRLNet(64 ,10)\n","\n","matrice =[]\n","lab =[]\n","m_train=[]\n","l_train = []\n","best_acc=[]\n","\n","\n","for s in range(0, total_classes, num_classes):\n","\n","    train_loader = torch.utils.data.DataLoader(decine_train[int((s/10))], batch_size=batch_size,shuffle=True, num_workers=4)\n","    test_loader = torch.utils.data.DataLoader(decine_val[int((s/10))], batch_size=batch_size,shuffle=True, num_workers=4)\n","    \n","    # Update representation via BackProp\n","    t1,t2 = icarl.update_representation(decine_train[int((s/10))], train_dataset)\n","\n","    m_train.append(t2)\n","    l_train.append(t1)\n","\n","\n","    # Construct exemplar sets for new classes\n","    for y in range(icarl.n_known, icarl.n_known+10):\n","        print(\"Constructing exemplar set for class-%d...\" %(y))\n","        indici, images = class_images(decine_train[int((s/10))],y)\n","        icarl.construct_exemplar_set(images,indici, transform = None)\n","        print(\"Done\")\n","    \n","    icarl.n_known = icarl.n_known+10\n","    print(\"iCaRL classes: %d\" % icarl.n_known)\n","    \n","    total = 0.0\n","    correct = 0.0\n","    with torch.no_grad():\n","     for indices, images, labels in train_loader:\n","         images = Variable(images).cuda()\n","         preds = icarl.classify(images, transform=None)\n","         total += labels.size(0)\n","         preds = torch.Tensor(preds)\n","         correct += (preds == labels).sum()\n","\n","    train_accuracy = float(correct/total)\n","    print('Train Accuracy: %f %%' % (train_accuracy))\n","  \n","    med_matr = []\n","    med_lab = []\n","    total = 0.0\n","    correct = 0.0\n","    with torch.no_grad():\n","     for indices, images, labels in test_loader:\n","        images = Variable(images).cuda()\n","        preds = icarl.classify(images, transform=None)\n","        total += labels.size(0)\n","        preds = torch.Tensor(preds)\n","        correct += (preds == labels).sum()\n","        med_lab.extend(labels)\n","        med_matr.extend(preds)\n","    \n","    test_accuracy = float(correct/total)\n","    matrice.append(med_matr)\n","    lab.append(med_lab)\n","    best_acc.append(test_accuracy)\n","    print('Test Accuracy: %f %%' % (test_accuracy))\n","    \n","    total = 0.0\n","    correct = 0.0\n","    with torch.no_grad():\n","     for indices, images, labels in test_loader:\n","        images = Variable(images).cuda()\n","        labels = Variable(labels).cuda()\n","\n","        g = icarl.forward(images)\n","        _, preds = torch.max(g, 1)\n","        correct += torch.sum(preds == labels.data).data.item()\n","        \n","        total += labels.size(0)\n","        accuracy = float(correct/float(total))\n","\n","    print(accuracy)\n","\n","    m = K/icarl.n_classes\n","    m = int(m)\n","    icarl.reduce_exemplar_sets(m)\n","    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["5000\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["0 new classes\n","LR: [2]\n","Epoch [1/70], Loss: 0.3361, Acc: 0.10 \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eHvUuR-stzj4","colab_type":"code","colab":{}},"source":["print(icarl.exemplar_variance[0])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lZLxcnZsLYNg","colab_type":"text"},"source":["**Confusion Matrix**"]},{"cell_type":"code","metadata":{"id":"zJ7B5LYvbju5","colab_type":"code","colab":{}},"source":["from sklearn.metrics import confusion_matrix\n","import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","print(len(matrice))\n","\n","x = matrice[9]\n","l = lab[9]\n","\n","#print()\n","#print(matrice[0])\n","\n","l = [int(i) for i in l]\n","x = [int(i) for i in x]\n","\n","cf = confusion_matrix(x,l)\n","\n","fig, ax = plt.subplots(figsize=(15,13))\n","im = ax.imshow(cf,cmap='plasma')\n","\n","ax.set_yticks([10,20,30,40,50,60,70,80,90])\n","ax.set_xticks([10,20,30,40,50,60,70,80,90])\n","\n","plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n","         rotation_mode=\"anchor\")\n","\n","ax.set_title(\"image classification\")\n","fig.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6AJmf1CuLdWD","colab_type":"text"},"source":["**Accuracy trend plot**"]},{"cell_type":"code","metadata":{"id":"bJEHzlQ2ogfu","colab_type":"code","colab":{}},"source":["fig,ax = plt.subplots(figsize=(15,10))\n","ax.plot([10,20,30,40,50,60,70,80,90,100],best_acc,'k-o')\n","plt.xlim(10,100)\n","plt.ylim(0,1)\n","plt.xlabel('Classes')\n","plt.ylabel('Accuracy')\n","plt.show()\n","\n","print(best_acc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N9RUmO_zLhZK","colab_type":"text"},"source":["**Storing results**"]},{"cell_type":"code","metadata":{"id":"F0SzqcjAp0ze","colab_type":"code","colab":{}},"source":["results_file = open(\"Results.txt\",\"a\")\n","\n","results_file.write(\"accuracy:\")\n","results_file.write(str(best_acc))\n","\n","np.savetxt(\"CF_ICARL.txt\",cf,delimiter = ',')\n","\n","results_file.close()"],"execution_count":null,"outputs":[]}]}