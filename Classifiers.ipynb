{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/armando-larocca/Project-IL-/blob/master/Classifiers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI5U_VjS82Oq",
        "colab_type": "text"
      },
      "source": [
        "**Import utilities**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38UuA0Ubj-Ku",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c283ce40-1a08-47f5-d932-1c9cc87a01ca"
      },
      "source": [
        "import torch\n",
        "import random\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch.optim as optim \n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from torch.backends import * \n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "DEVICE = torch.device('cuda:0')\n",
        "\n",
        "if not os.path.isdir('./Project-dir'):\n",
        "  !git clone https://github.com/armando-larocca/Project-IL-\n",
        "\n",
        "if not os.path.isdir('content/cifar100.py'):\n",
        "  !mv '/content/Project-IL-/cifar100.py' '/content'  \n",
        "  !mv '/content/Project-IL-/utils.py' '/content'  \n",
        "\n",
        "if not os.path.isdir('content/cifarResnet.py'):\n",
        "  !mv '/content/Project-IL-/cifarResnet.py' '/content'\n",
        "\n",
        "if not os.path.isdir('content/icarl_utils.py'):\n",
        "  !mv '/content/Project-IL-/icarl_utils.py' '/content'  \n",
        "\n",
        "from cifarResnet import resnet32\n",
        "from cifar100 import *\n",
        "from icarl_utils import * \n",
        "\n",
        "random.seed(653)\n",
        "# Hyper Parameters\n",
        "num_epochs = 70\n",
        "batch_size = 128\n",
        "total_classes = 100\n",
        "num_classes = 10\n",
        "indici_classi = list(range(0,100))\n",
        "random.shuffle(indici_classi)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'Project-IL-' already exists and is not an empty directory.\n",
            "mv: cannot stat '/content/Project-IL-/cifar100.py': No such file or directory\n",
            "mv: cannot stat '/content/Project-IL-/utils.py': No such file or directory\n",
            "mv: cannot stat '/content/Project-IL-/cifarResnet.py': No such file or directory\n",
            "mv: cannot stat '/content/Project-IL-/icarl_utils.py': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CdFb8elLEJG",
        "colab_type": "text"
      },
      "source": [
        "**Weights initialization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omjdWY74ddV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_weights(m):\n",
        "  if type(m) == nn.Conv2d:\n",
        "    torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
        "\n",
        "  if type(m) == nn.Linear:\n",
        "    torch.nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcsTwgvOLIrm",
        "colab_type": "text"
      },
      "source": [
        "**ICARL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jyhGkpp8xjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class iCaRLNet(nn.Module):\n",
        "    def __init__(self, feature_size, n_classes):\n",
        "        \n",
        "        #### Network architecture ####Â \n",
        "        super(iCaRLNet, self).__init__()\n",
        "        self.feature_extractor = resnet32()\n",
        "        self.fc = nn.Linear(feature_size, n_classes, bias=True)\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        self.n_known = 0\n",
        "\n",
        "        self.cls_loss = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        self.exemplar_sets = []\n",
        "        self.indexes = []\n",
        "\n",
        "        # Means of exemplars\n",
        "        self.compute_means = True\n",
        "        self.exemplar_means = []\n",
        "\n",
        "        self.apply(init_weights)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "    def increment_classes(self, n):\n",
        "        ### INCREMENT THE NUMBER OF OUTPUTS  ###\n",
        "        ### AND COPY THE WEIGHTS            ####\n",
        "        torch.no_grad()\n",
        "        self.train(False)\n",
        "        in_features = self.fc.in_features\n",
        "        out_features = self.fc.out_features\n",
        "        weight = self.fc.weight.data\n",
        "        bias = self.fc.bias.data\n",
        "\n",
        "        self.fc = nn.Linear(in_features, out_features+n, bias=True)\n",
        "        self.fc.weight.data[:out_features] = weight\n",
        "        self.fc.bias.data[:out_features] = bias\n",
        "        self.n_classes += n\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def MLP(self,n_neurons):\n",
        "      torch.no_grad()\n",
        "      self.train(False)\n",
        "\n",
        "      eta = 0 \n",
        "      features = []\n",
        "      lab = []\n",
        "      for P_y in self.exemplar_sets:\n",
        "        # Extract feature for each exemplar in P_y\n",
        "        for ex in P_y:\n",
        "          ex = Variable(ex, volatile=True).cuda()\n",
        "          feature = self.feature_extractor(ex.unsqueeze(0))\n",
        "          feature = feature.squeeze()\n",
        "          feature.data = feature.data / feature.data.norm() # Normalize\n",
        "          features.append(feature.cpu().detach().numpy())\n",
        "          lab.append(eta)\n",
        "        eta+=1\n",
        "\n",
        "      classifier = MLPClassifier(solver='sgd', learning_rate_init=0.1, random_state=1, hidden_layer_sizes=n_neurons)\n",
        "      classifier.fit(features, lab)\n",
        "\n",
        "      return classifier\n",
        "\n",
        "\n",
        "\n",
        "    def knn_classify(self,x):\n",
        "      torch.no_grad()\n",
        "      self.train(False)\n",
        "      batch_size = x.size(0)\n",
        "\n",
        "      eta = 0 \n",
        "      features = []\n",
        "      lab = []\n",
        "      for P_y in self.exemplar_sets:\n",
        "        # Extract feature for each exemplar in P_y\n",
        "        for ex in P_y:\n",
        "          ex = Variable(ex, volatile=True).cuda()\n",
        "          feature = self.feature_extractor(ex.unsqueeze(0))\n",
        "          feature = feature.squeeze()\n",
        "          feature.data = feature.data / feature.data.norm() # Normalize\n",
        "          features.append(feature.cpu().detach().numpy())\n",
        "          lab.append(eta)\n",
        "        eta+=1\n",
        "\n",
        "      classifier = KNeighborsClassifier(n_neighbors=1)\n",
        "      classifier.fit(features, lab)\n",
        "\n",
        "      y = self.feature_extractor(x) # (batch_size, feature_size)\n",
        "      for i in range(0,y.size(0)): # Normalize\n",
        "        y.data[i] = y.data[i] / y.data[i].norm()\n",
        "\n",
        "      y = y.cpu()\n",
        "      y = y.detach().numpy()\n",
        "\n",
        "      out = classifier.predict(y)\n",
        "\n",
        "      return out\n",
        "\n",
        "\n",
        "\n",
        "    def classify(self, x):\n",
        "        \"\"\"Classify images by neares-means-of-exemplars\n",
        "        Args:\n",
        "            x: input image batch\n",
        "        Returns:\n",
        "            preds: Tensor of size (batch_size,)\n",
        "        \"\"\"\n",
        "        torch.no_grad()\n",
        "        self.train(False)\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        if self.compute_means:\n",
        "            print (\"Computing mean of exemplars...\")\n",
        "            exemplar_means = []\n",
        "            for P_y in self.exemplar_sets:\n",
        "                features = []\n",
        "                # Extract feature for each exemplar in P_y\n",
        "                for ex in P_y:\n",
        "                    ex = Variable(ex, volatile=True).cuda()\n",
        "                    feature = self.feature_extractor(ex.unsqueeze(0))\n",
        "                    feature = feature.squeeze()\n",
        "                    feature.data = feature.data / feature.data.norm() # Normalize\n",
        "                    features.append(feature)\n",
        "                features = torch.stack(features)\n",
        "                mu_y = features.mean(0).squeeze()\n",
        "                mu_y.data = mu_y.data / mu_y.data.norm() # Normalize\n",
        "                exemplar_means.append(mu_y)\n",
        "            self.exemplar_means = exemplar_means\n",
        "            self.compute_means = False\n",
        "            print (\"Done\")\n",
        "\n",
        "        exemplar_means = self.exemplar_means\n",
        "        means = torch.stack(exemplar_means) # (n_classes, feature_size)\n",
        "        means = torch.stack([means] * batch_size) # (batch_size, n_classes, feature_size)\n",
        "        means = means.transpose(1, 2) # (batch_size, feature_size, n_classes)\n",
        "\n",
        "        feature = self.feature_extractor(x) # (batch_size, feature_size)\n",
        "        for i in range(0,feature.size(0)): # Normalize\n",
        "            feature.data[i] = feature.data[i] / feature.data[i].norm()\n",
        "        feature = feature.unsqueeze(2) # (batch_size, feature_size, 1)\n",
        "        feature = feature.expand_as(means) # (batch_size, feature_size, n_classes)\n",
        "\n",
        "        dists = (feature - means).pow(2).sum(1).squeeze() #(batch_size, n_classes)\n",
        "        _, preds = dists.min(1)\n",
        "\n",
        "        return preds\n",
        "        \n",
        "\n",
        "\n",
        "    def construct_exemplar_set(self, images, indicis):\n",
        "  \n",
        "        \"\"\"Construct an exemplar set for image set\n",
        "        Args:\n",
        "            images: np.array containing images of a class\n",
        "        \"\"\"\n",
        "        torch.no_grad()\n",
        "        self.train(False)\n",
        "        # Compute and cache features for each example\n",
        "        features = []\n",
        "        for img in images:\n",
        "            x = Variable(img, volatile=True).cuda()\n",
        "            feature = self.feature_extractor(x.unsqueeze(0)).data.cpu().numpy()\n",
        "            feature = feature / np.linalg.norm(feature) # Normalize\n",
        "            features.append(feature[0])\n",
        "\n",
        "        features = np.array(features)\n",
        "        class_mean = np.mean(features, axis=0)\n",
        "        class_mean = class_mean / np.linalg.norm(class_mean) # Normalize\n",
        "\n",
        "        exemplar_set = []\n",
        "        exemplar_features = [] # list of Variables of shape (feature_size,)\n",
        "        indici = []\n",
        "        for k in range(0, len(images)):\n",
        "            S = np.sum(exemplar_features, axis=0)\n",
        "            phi = features\n",
        "            mu = class_mean\n",
        "            mu_p = 1.0/(k+1) * (phi + S)\n",
        "            mu_p = mu_p / np.linalg.norm(mu_p)\n",
        "            i = np.argmin(np.sqrt(np.sum((mu - mu_p) ** 2, axis=1)))\n",
        "            exemplar_set.append(images[i])\n",
        "            indici.append(indicis[i])\n",
        "            exemplar_features.append(features[i])\n",
        "          \n",
        "            #print (\"Selected example\", i)\n",
        "            #print (\"|exemplar_mean - class_mean|:\")\n",
        "            #print (np.linalg.norm((np.mean(exemplar_features, axis=0) - class_mean)))\n",
        "            features = np.delete(features, i, axis=0)\n",
        "            \n",
        "        #exemplar_set = torch.stack(exemplar_set)\n",
        "        self.exemplar_sets.append(exemplar_set)\n",
        "        self.indexes.append(indici)        \n",
        "\n",
        "\n",
        "    def reduce_exemplar_sets(self, m):\n",
        "        self.train(False)\n",
        "        for y, P_y in enumerate(self.exemplar_sets):\n",
        "            self.exemplar_sets[y] = P_y[:int(m)]\n",
        "            uff = self.indexes[y] \n",
        "            uff = uff[:int(m)]\n",
        "            self.indexes[y] = uff\n",
        "\n",
        "\n",
        "\n",
        "    def combine_dataset_with_exemplars(self, dataset, superdataset):\n",
        "        self.train(False)\n",
        "        indici = []\n",
        "        a = list(dataset)\n",
        "        for item in a:\n",
        "          indici.append(item[0])\n",
        "        for j in self.indexes:\n",
        "           indici = indici + j\n",
        "        print(len(indici))\n",
        "        dataset = Subset(superdataset, indici)\n",
        "\n",
        "        return dataset\n",
        "            \n",
        "        \n",
        "\n",
        "\n",
        "    def update_representation(self, dataset, superdataset):\n",
        "\n",
        "        self.cuda()\n",
        "        p = icarl.parameters()\n",
        "        optimizer = optim.SGD(p, lr=2, weight_decay = 0.00001, momentum=0.9)\n",
        "        #self.cuda()\n",
        "        \n",
        "        dataset = self.combine_dataset_with_exemplars(dataset, superdataset)         \n",
        "        loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,shuffle=True, num_workers=4)\n",
        "\n",
        "        self.compute_means = True\n",
        "        self.train(False)\n",
        "        \n",
        "        # Store network outputs with pre-update parameters\n",
        "        self.train(False)\n",
        "        q = torch.zeros(52000, self.n_classes).cuda()\n",
        "        for  indices, images, labels in loader:\n",
        "            images = Variable(images).cuda()\n",
        "            indices = indices.cuda()\n",
        "            g = F.sigmoid(self.forward(images))\n",
        "            q[indices] = g.data         \n",
        "        q = Variable(q).cuda()\n",
        "\n",
        "        \n",
        "        # Increment number of weights in final fc layer\n",
        "        classes = list(set(lb(dataset)))\n",
        "        new_classes = [cls for cls in classes if cls > self.n_classes - 1]\n",
        "\n",
        "        if(len(new_classes) != 0):\n",
        "          print(\"#####################\")\n",
        "          self.increment_classes(len(new_classes))\n",
        "          self.cuda()\n",
        "          p = self.parameters() \n",
        "          optimizer = optim.SGD(p, lr=2.0, weight_decay = 0.00001, momentum=0.9)\n",
        "        #self.cuda()\n",
        "        print (\"%d new classes\" % (len(new_classes)))\n",
        "        \n",
        "        tot_matr =[]\n",
        "        tot_lab=[]\n",
        "        \n",
        "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [49,63], gamma=0.2)\n",
        "    \n",
        "        for epoch in range(0,num_epochs):\n",
        "            running_corrects = 0\n",
        "            total = 0\n",
        "            matr =[]\n",
        "            lab =[]\n",
        "             \n",
        "            for i, (indices, images, labels) in enumerate(loader):\n",
        "        \n",
        "                images = Variable(images).cuda()\n",
        "                labels = Variable(labels).cuda()\n",
        "                indices = indices.cuda()\n",
        "\n",
        "                self.train(True)\n",
        "                optimizer.zero_grad()\n",
        "                g = self.forward(images)\n",
        "                \n",
        "                _, preds = torch.max(g, 1)\n",
        "                running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "                matr.extend(preds)\n",
        "                lab.extend(labels)\n",
        "\n",
        "                q_i = q[indices]\n",
        "                Id = torch.eye(self.n_known+10)\n",
        "                one_hot = Id[labels].cuda()\n",
        "                one_hot[:,:self.n_known] = q_i[:,:self.n_known]\n",
        "                loss = self.cls_loss(g, one_hot) \n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            self.train(False)\n",
        "            tot_lab.append(lab)\n",
        "            tot_matr.append(matr)\n",
        "\n",
        "            scheduler.step()\n",
        "            accuracy = running_corrects / float(total)\n",
        "          \n",
        "            print(\"LR:\",scheduler.get_last_lr())\n",
        "            print ('Epoch [%d/%d], Loss: %.4f, Acc: %.2f ' %(epoch+1, num_epochs, loss.data, accuracy,)) \n",
        "\n",
        "        return tot_lab, tot_matr      \n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCYSysgDOhXM",
        "colab_type": "text"
      },
      "source": [
        "**Dataset preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ui_OaQofk04A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "9d1c3ce2-e627-41e9-fdbc-13a403adeaae"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(0.5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "])\n",
        "\n",
        "train_dataset = Cifar100(\".\\Data\", train=True, transform=transform)\n",
        "test_dataset = Cifar100(\".\\Data\", train=False, transform=transform_test)\n",
        "\n",
        "\n",
        "train_dataset._shuffle_(indici_classi)\n",
        "test_dataset._shuffle_(indici_classi)\n",
        "\n",
        "incr_train = train_dataset.__incremental_train_indexes__(1)\n",
        "incr_val = test_dataset.__incremental_val_indexes__(0)\n",
        "\n",
        "decine_train = []\n",
        "decine_val = []\n",
        "\n",
        "for i in range(0,10):\n",
        "  val_dataset = Subset(test_dataset, incr_val[i])\n",
        "  training_dataset = Subset(train_dataset, incr_train[i])\n",
        "  decine_train.append(training_dataset)\n",
        "  decine_val.append(val_dataset) \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: .\\Data/cifar-100-python.tar.gz\n",
            "Extracting .\\Data/cifar-100-python.tar.gz to .\\Data\n",
            "Using downloaded and verified file: .\\Data/cifar-100-python.tar.gz\n",
            "Extracting .\\Data/cifar-100-python.tar.gz to .\\Data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6R5he6AELS7j",
        "colab_type": "text"
      },
      "source": [
        "**Main**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzq10DaWMfDt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ecbe789b-7f07-47bb-c1f7-64fb36d4a466"
      },
      "source": [
        "# Initialize CNN\n",
        "K = 2000 # total number of exemplars\n",
        "icarl = iCaRLNet(64 ,10)\n",
        "\n",
        "matrice =[]\n",
        "lab =[]\n",
        "m_train=[]\n",
        "l_train = []\n",
        "best_acc=[]\n",
        "\n",
        "\n",
        "for s in range(0, total_classes, num_classes):\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(decine_train[int((s/10))], batch_size=batch_size,shuffle=True, num_workers=4)\n",
        "    test_loader = torch.utils.data.DataLoader(decine_val[int((s/10))], batch_size=batch_size,shuffle=True, num_workers=4)\n",
        "    \n",
        "    t1,t2 = icarl.update_representation(decine_train[int((s/10))], train_dataset)\n",
        "\n",
        "    m_train.append(t2)\n",
        "    l_train.append(t1)\n",
        "    \n",
        "    # Construct exemplar sets for new classes\n",
        "    for y in range(icarl.n_known, icarl.n_known+10):\n",
        "        print(\"Constructing exemplar set for class-%d...\" %(y))\n",
        "        indici, images = class_images(decine_train[int((s/10))],y)\n",
        "        icarl.construct_exemplar_set(images,indici, transform = None)\n",
        "        print(\"Done\")\n",
        "    \n",
        "    icarl.n_known = icarl.n_known+10\n",
        "    print(\"iCaRL classes: %d\" % icarl.n_known)\n",
        "\n",
        "    # Reduce exemplar sets for known classes\n",
        "    m = K/icarl.n_classes\n",
        "    icarl.reduce_exemplar_sets(m)\n",
        "\n",
        "    med_matr = []\n",
        "    med_lab = []\n",
        "    total = 0.0\n",
        "    correct = 0.0\n",
        "\n",
        "    n_neurons = int((64 + icarl.n_known)/2)\n",
        "\n",
        "    classifier = icarl.MLP(n_neurons)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "     for indices, images, labels in test_loader:\n",
        "        images = Variable(images).cuda()\n",
        "\n",
        "        y = icarl.feature_extractor(images) # (batch_size, feature_size)\n",
        "        for i in range(0,y.size(0)): # Normalize\n",
        "          y.data[i] = y.data[i] / y.data[i].norm()\n",
        "\n",
        "        y = y.cpu()\n",
        "        y = y.detach().numpy()\n",
        "        preds = classifier.predict(y)\n",
        "      \n",
        "        total += labels.size(0)        \n",
        "        for i in range(0,len(preds)):\n",
        "          if(preds[i] == int(labels[i]) ):\n",
        "            correct += 1\n",
        "        med_lab.extend(labels)\n",
        "        med_matr.extend(preds)\n",
        "    \n",
        "    test_accuracy = float(correct/total)\n",
        "    matrice.append(med_matr)\n",
        "    lab.append(med_lab)\n",
        "    best_acc.append(test_accuracy)\n",
        "    print('Test Accuracy: %f %%' % (test_accuracy))\n",
        "    \n",
        "    total = 0.0\n",
        "    correct = 0.0\n",
        "    with torch.no_grad():\n",
        "     for indices, images, labels in test_loader:\n",
        "        images = Variable(images).cuda()\n",
        "        labels = Variable(labels).cuda()\n",
        "\n",
        "        g = icarl.forward(images)\n",
        "        _, preds = torch.max(g, 1)\n",
        "        correct += torch.sum(preds == labels.data).data.item()\n",
        "        \n",
        "        total += labels.size(0)\n",
        "        accuracy = float(correct/float(total))\n",
        "\n",
        "    print(accuracy)\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 new classes\n",
            "LR: [2]\n",
            "Epoch [1/70], Loss: 0.3610, Acc: 0.11 \n",
            "LR: [2]\n",
            "Epoch [2/70], Loss: 0.3266, Acc: 0.10 \n",
            "LR: [2]\n",
            "Epoch [3/70], Loss: 0.3371, Acc: 0.14 \n",
            "LR: [2]\n",
            "Epoch [4/70], Loss: 0.3149, Acc: 0.14 \n",
            "LR: [2]\n",
            "Epoch [5/70], Loss: 0.3441, Acc: 0.20 \n",
            "LR: [2]\n",
            "Epoch [6/70], Loss: 0.3508, Acc: 0.25 \n",
            "LR: [2]\n",
            "Epoch [7/70], Loss: 0.3456, Acc: 0.26 \n",
            "LR: [2]\n",
            "Epoch [8/70], Loss: 0.2153, Acc: 0.32 \n",
            "LR: [2]\n",
            "Epoch [9/70], Loss: 0.2162, Acc: 0.34 \n",
            "LR: [2]\n",
            "Epoch [10/70], Loss: 0.3102, Acc: 0.37 \n",
            "LR: [2]\n",
            "Epoch [11/70], Loss: 0.2018, Acc: 0.39 \n",
            "LR: [2]\n",
            "Epoch [12/70], Loss: 0.2154, Acc: 0.41 \n",
            "LR: [2]\n",
            "Epoch [13/70], Loss: 0.2537, Acc: 0.40 \n",
            "LR: [2]\n",
            "Epoch [14/70], Loss: 0.2821, Acc: 0.43 \n",
            "LR: [2]\n",
            "Epoch [15/70], Loss: 0.2647, Acc: 0.44 \n",
            "LR: [2]\n",
            "Epoch [16/70], Loss: 0.2197, Acc: 0.46 \n",
            "LR: [2]\n",
            "Epoch [17/70], Loss: 0.2699, Acc: 0.47 \n",
            "LR: [2]\n",
            "Epoch [18/70], Loss: 0.2927, Acc: 0.47 \n",
            "LR: [2]\n",
            "Epoch [19/70], Loss: 0.2862, Acc: 0.49 \n",
            "LR: [2]\n",
            "Epoch [20/70], Loss: 0.2740, Acc: 0.50 \n",
            "LR: [2]\n",
            "Epoch [21/70], Loss: 0.3183, Acc: 0.52 \n",
            "LR: [2]\n",
            "Epoch [22/70], Loss: 0.2730, Acc: 0.52 \n",
            "LR: [2]\n",
            "Epoch [23/70], Loss: 0.2470, Acc: 0.54 \n",
            "LR: [2]\n",
            "Epoch [24/70], Loss: 0.2344, Acc: 0.56 \n",
            "LR: [2]\n",
            "Epoch [25/70], Loss: 0.2128, Acc: 0.55 \n",
            "LR: [2]\n",
            "Epoch [26/70], Loss: 0.1236, Acc: 0.58 \n",
            "LR: [2]\n",
            "Epoch [27/70], Loss: 0.1951, Acc: 0.60 \n",
            "LR: [2]\n",
            "Epoch [28/70], Loss: 0.3378, Acc: 0.62 \n",
            "LR: [2]\n",
            "Epoch [29/70], Loss: 0.2691, Acc: 0.60 \n",
            "LR: [2]\n",
            "Epoch [30/70], Loss: 0.1928, Acc: 0.61 \n",
            "LR: [2]\n",
            "Epoch [31/70], Loss: 0.2829, Acc: 0.62 \n",
            "LR: [2]\n",
            "Epoch [32/70], Loss: 0.2476, Acc: 0.63 \n",
            "LR: [2]\n",
            "Epoch [33/70], Loss: 0.1408, Acc: 0.65 \n",
            "LR: [2]\n",
            "Epoch [34/70], Loss: 0.1751, Acc: 0.68 \n",
            "LR: [2]\n",
            "Epoch [35/70], Loss: 0.2457, Acc: 0.69 \n",
            "LR: [2]\n",
            "Epoch [36/70], Loss: 0.1656, Acc: 0.70 \n",
            "LR: [2]\n",
            "Epoch [37/70], Loss: 0.2134, Acc: 0.70 \n",
            "LR: [2]\n",
            "Epoch [38/70], Loss: 0.3515, Acc: 0.70 \n",
            "LR: [2]\n",
            "Epoch [39/70], Loss: 0.1569, Acc: 0.70 \n",
            "LR: [2]\n",
            "Epoch [40/70], Loss: 0.2405, Acc: 0.74 \n",
            "LR: [2]\n",
            "Epoch [41/70], Loss: 0.1866, Acc: 0.73 \n",
            "LR: [2]\n",
            "Epoch [42/70], Loss: 0.2272, Acc: 0.72 \n",
            "LR: [2]\n",
            "Epoch [43/70], Loss: 0.1802, Acc: 0.74 \n",
            "LR: [2]\n",
            "Epoch [44/70], Loss: 0.1220, Acc: 0.74 \n",
            "LR: [2]\n",
            "Epoch [45/70], Loss: 0.2645, Acc: 0.76 \n",
            "LR: [2]\n",
            "Epoch [46/70], Loss: 0.3457, Acc: 0.73 \n",
            "LR: [2]\n",
            "Epoch [47/70], Loss: 0.2084, Acc: 0.75 \n",
            "LR: [2]\n",
            "Epoch [48/70], Loss: 0.1784, Acc: 0.76 \n",
            "LR: [0.4]\n",
            "Epoch [49/70], Loss: 0.1448, Acc: 0.79 \n",
            "LR: [0.4]\n",
            "Epoch [50/70], Loss: 0.0694, Acc: 0.83 \n",
            "LR: [0.4]\n",
            "Epoch [51/70], Loss: 0.2393, Acc: 0.85 \n",
            "LR: [0.4]\n",
            "Epoch [52/70], Loss: 0.1427, Acc: 0.85 \n",
            "LR: [0.4]\n",
            "Epoch [53/70], Loss: 0.0910, Acc: 0.85 \n",
            "LR: [0.4]\n",
            "Epoch [54/70], Loss: 0.1128, Acc: 0.86 \n",
            "LR: [0.4]\n",
            "Epoch [55/70], Loss: 0.0736, Acc: 0.87 \n",
            "LR: [0.4]\n",
            "Epoch [56/70], Loss: 0.0974, Acc: 0.87 \n",
            "LR: [0.4]\n",
            "Epoch [57/70], Loss: 0.0830, Acc: 0.88 \n",
            "LR: [0.4]\n",
            "Epoch [58/70], Loss: 0.1859, Acc: 0.87 \n",
            "LR: [0.4]\n",
            "Epoch [59/70], Loss: 0.0999, Acc: 0.88 \n",
            "LR: [0.4]\n",
            "Epoch [60/70], Loss: 0.1114, Acc: 0.89 \n",
            "LR: [0.4]\n",
            "Epoch [61/70], Loss: 0.0917, Acc: 0.89 \n",
            "LR: [0.4]\n",
            "Epoch [62/70], Loss: 0.0818, Acc: 0.89 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [63/70], Loss: 0.3799, Acc: 0.89 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [64/70], Loss: 0.1122, Acc: 0.90 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [65/70], Loss: 0.0302, Acc: 0.91 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [66/70], Loss: 0.1092, Acc: 0.91 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [67/70], Loss: 0.0542, Acc: 0.92 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [68/70], Loss: 0.0475, Acc: 0.92 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [69/70], Loss: 0.1107, Acc: 0.92 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [70/70], Loss: 0.1456, Acc: 0.92 \n",
            "Constructing exemplar set for class-0...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:172: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done\n",
            "Constructing exemplar set for class-1...\n",
            "Done\n",
            "Constructing exemplar set for class-2...\n",
            "Done\n",
            "Constructing exemplar set for class-3...\n",
            "Done\n",
            "Constructing exemplar set for class-4...\n",
            "Done\n",
            "Constructing exemplar set for class-5...\n",
            "Done\n",
            "Constructing exemplar set for class-6...\n",
            "Done\n",
            "Constructing exemplar set for class-7...\n",
            "Done\n",
            "Constructing exemplar set for class-8...\n",
            "Done\n",
            "Constructing exemplar set for class-9...\n",
            "Done\n",
            "iCaRL classes: 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:62: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.792000 %\n",
            "0.797\n",
            "7000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "#####################\n",
            "10 new classes\n",
            "LR: [2.0]\n",
            "Epoch [1/70], Loss: 0.1697, Acc: 0.27 \n",
            "LR: [2.0]\n",
            "Epoch [2/70], Loss: 0.1423, Acc: 0.38 \n",
            "LR: [2.0]\n",
            "Epoch [3/70], Loss: 0.1372, Acc: 0.45 \n",
            "LR: [2.0]\n",
            "Epoch [4/70], Loss: 0.1327, Acc: 0.52 \n",
            "LR: [2.0]\n",
            "Epoch [5/70], Loss: 0.1376, Acc: 0.55 \n",
            "LR: [2.0]\n",
            "Epoch [6/70], Loss: 0.1359, Acc: 0.59 \n",
            "LR: [2.0]\n",
            "Epoch [7/70], Loss: 0.1491, Acc: 0.61 \n",
            "LR: [2.0]\n",
            "Epoch [8/70], Loss: 0.1268, Acc: 0.63 \n",
            "LR: [2.0]\n",
            "Epoch [9/70], Loss: 0.1255, Acc: 0.64 \n",
            "LR: [2.0]\n",
            "Epoch [10/70], Loss: 0.1248, Acc: 0.66 \n",
            "LR: [2.0]\n",
            "Epoch [11/70], Loss: 0.1274, Acc: 0.68 \n",
            "LR: [2.0]\n",
            "Epoch [12/70], Loss: 0.1181, Acc: 0.69 \n",
            "LR: [2.0]\n",
            "Epoch [13/70], Loss: 0.1148, Acc: 0.70 \n",
            "LR: [2.0]\n",
            "Epoch [14/70], Loss: 0.1329, Acc: 0.72 \n",
            "LR: [2.0]\n",
            "Epoch [15/70], Loss: 0.1380, Acc: 0.72 \n",
            "LR: [2.0]\n",
            "Epoch [16/70], Loss: 0.1104, Acc: 0.73 \n",
            "LR: [2.0]\n",
            "Epoch [17/70], Loss: 0.1365, Acc: 0.74 \n",
            "LR: [2.0]\n",
            "Epoch [18/70], Loss: 0.1199, Acc: 0.74 \n",
            "LR: [2.0]\n",
            "Epoch [19/70], Loss: 0.1194, Acc: 0.75 \n",
            "LR: [2.0]\n",
            "Epoch [20/70], Loss: 0.1184, Acc: 0.76 \n",
            "LR: [2.0]\n",
            "Epoch [21/70], Loss: 0.1275, Acc: 0.77 \n",
            "LR: [2.0]\n",
            "Epoch [22/70], Loss: 0.1137, Acc: 0.77 \n",
            "LR: [2.0]\n",
            "Epoch [23/70], Loss: 0.1013, Acc: 0.77 \n",
            "LR: [2.0]\n",
            "Epoch [24/70], Loss: 0.0940, Acc: 0.78 \n",
            "LR: [2.0]\n",
            "Epoch [25/70], Loss: 0.0977, Acc: 0.78 \n",
            "LR: [2.0]\n",
            "Epoch [26/70], Loss: 0.1024, Acc: 0.79 \n",
            "LR: [2.0]\n",
            "Epoch [27/70], Loss: 0.1101, Acc: 0.79 \n",
            "LR: [2.0]\n",
            "Epoch [28/70], Loss: 0.1060, Acc: 0.80 \n",
            "LR: [2.0]\n",
            "Epoch [29/70], Loss: 0.1040, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [30/70], Loss: 0.1245, Acc: 0.82 \n",
            "LR: [2.0]\n",
            "Epoch [31/70], Loss: 0.0838, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [32/70], Loss: 0.1174, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [33/70], Loss: 0.0971, Acc: 0.82 \n",
            "LR: [2.0]\n",
            "Epoch [34/70], Loss: 0.0943, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [35/70], Loss: 0.1125, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [36/70], Loss: 0.1071, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [37/70], Loss: 0.0909, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [38/70], Loss: 0.0952, Acc: 0.84 \n",
            "LR: [2.0]\n",
            "Epoch [39/70], Loss: 0.0816, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [40/70], Loss: 0.1065, Acc: 0.84 \n",
            "LR: [2.0]\n",
            "Epoch [41/70], Loss: 0.1057, Acc: 0.84 \n",
            "LR: [2.0]\n",
            "Epoch [42/70], Loss: 0.1013, Acc: 0.85 \n",
            "LR: [2.0]\n",
            "Epoch [43/70], Loss: 0.0884, Acc: 0.86 \n",
            "LR: [2.0]\n",
            "Epoch [44/70], Loss: 0.1103, Acc: 0.85 \n",
            "LR: [2.0]\n",
            "Epoch [45/70], Loss: 0.0764, Acc: 0.85 \n",
            "LR: [2.0]\n",
            "Epoch [46/70], Loss: 0.0893, Acc: 0.86 \n",
            "LR: [2.0]\n",
            "Epoch [47/70], Loss: 0.0959, Acc: 0.86 \n",
            "LR: [2.0]\n",
            "Epoch [48/70], Loss: 0.0998, Acc: 0.86 \n",
            "LR: [0.4]\n",
            "Epoch [49/70], Loss: 0.0993, Acc: 0.86 \n",
            "LR: [0.4]\n",
            "Epoch [50/70], Loss: 0.0805, Acc: 0.90 \n",
            "LR: [0.4]\n",
            "Epoch [51/70], Loss: 0.0737, Acc: 0.92 \n",
            "LR: [0.4]\n",
            "Epoch [52/70], Loss: 0.0720, Acc: 0.93 \n",
            "LR: [0.4]\n",
            "Epoch [53/70], Loss: 0.0724, Acc: 0.93 \n",
            "LR: [0.4]\n",
            "Epoch [54/70], Loss: 0.0806, Acc: 0.93 \n",
            "LR: [0.4]\n",
            "Epoch [55/70], Loss: 0.0767, Acc: 0.93 \n",
            "LR: [0.4]\n",
            "Epoch [56/70], Loss: 0.0830, Acc: 0.94 \n",
            "LR: [0.4]\n",
            "Epoch [57/70], Loss: 0.0650, Acc: 0.94 \n",
            "LR: [0.4]\n",
            "Epoch [58/70], Loss: 0.0785, Acc: 0.94 \n",
            "LR: [0.4]\n",
            "Epoch [59/70], Loss: 0.0681, Acc: 0.94 \n",
            "LR: [0.4]\n",
            "Epoch [60/70], Loss: 0.0802, Acc: 0.95 \n",
            "LR: [0.4]\n",
            "Epoch [61/70], Loss: 0.0764, Acc: 0.95 \n",
            "LR: [0.4]\n",
            "Epoch [62/70], Loss: 0.0745, Acc: 0.94 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [63/70], Loss: 0.0812, Acc: 0.95 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [64/70], Loss: 0.0712, Acc: 0.95 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [65/70], Loss: 0.0725, Acc: 0.95 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [66/70], Loss: 0.0690, Acc: 0.95 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [67/70], Loss: 0.0768, Acc: 0.95 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [68/70], Loss: 0.0644, Acc: 0.95 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [69/70], Loss: 0.0739, Acc: 0.95 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [70/70], Loss: 0.0866, Acc: 0.95 \n",
            "Constructing exemplar set for class-10...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:172: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done\n",
            "Constructing exemplar set for class-11...\n",
            "Done\n",
            "Constructing exemplar set for class-12...\n",
            "Done\n",
            "Constructing exemplar set for class-13...\n",
            "Done\n",
            "Constructing exemplar set for class-14...\n",
            "Done\n",
            "Constructing exemplar set for class-15...\n",
            "Done\n",
            "Constructing exemplar set for class-16...\n",
            "Done\n",
            "Constructing exemplar set for class-17...\n",
            "Done\n",
            "Constructing exemplar set for class-18...\n",
            "Done\n",
            "Constructing exemplar set for class-19...\n",
            "Done\n",
            "iCaRL classes: 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:62: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.753000 %\n",
            "0.7605\n",
            "7000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "#####################\n",
            "10 new classes\n",
            "LR: [2.0]\n",
            "Epoch [1/70], Loss: 0.1389, Acc: 0.27 \n",
            "LR: [2.0]\n",
            "Epoch [2/70], Loss: 0.1262, Acc: 0.34 \n",
            "LR: [2.0]\n",
            "Epoch [3/70], Loss: 0.1338, Acc: 0.41 \n",
            "LR: [2.0]\n",
            "Epoch [4/70], Loss: 0.1169, Acc: 0.48 \n",
            "LR: [2.0]\n",
            "Epoch [5/70], Loss: 0.1281, Acc: 0.51 \n",
            "LR: [2.0]\n",
            "Epoch [6/70], Loss: 0.1223, Acc: 0.55 \n",
            "LR: [2.0]\n",
            "Epoch [7/70], Loss: 0.1221, Acc: 0.57 \n",
            "LR: [2.0]\n",
            "Epoch [8/70], Loss: 0.1063, Acc: 0.60 \n",
            "LR: [2.0]\n",
            "Epoch [9/70], Loss: 0.1056, Acc: 0.61 \n",
            "LR: [2.0]\n",
            "Epoch [10/70], Loss: 0.1096, Acc: 0.64 \n",
            "LR: [2.0]\n",
            "Epoch [11/70], Loss: 0.1048, Acc: 0.65 \n",
            "LR: [2.0]\n",
            "Epoch [12/70], Loss: 0.1176, Acc: 0.67 \n",
            "LR: [2.0]\n",
            "Epoch [13/70], Loss: 0.1158, Acc: 0.68 \n",
            "LR: [2.0]\n",
            "Epoch [14/70], Loss: 0.1018, Acc: 0.70 \n",
            "LR: [2.0]\n",
            "Epoch [15/70], Loss: 0.1006, Acc: 0.70 \n",
            "LR: [2.0]\n",
            "Epoch [16/70], Loss: 0.1086, Acc: 0.71 \n",
            "LR: [2.0]\n",
            "Epoch [17/70], Loss: 0.0998, Acc: 0.72 \n",
            "LR: [2.0]\n",
            "Epoch [18/70], Loss: 0.1039, Acc: 0.72 \n",
            "LR: [2.0]\n",
            "Epoch [19/70], Loss: 0.1126, Acc: 0.73 \n",
            "LR: [2.0]\n",
            "Epoch [20/70], Loss: 0.0958, Acc: 0.74 \n",
            "LR: [2.0]\n",
            "Epoch [21/70], Loss: 0.0997, Acc: 0.74 \n",
            "LR: [2.0]\n",
            "Epoch [22/70], Loss: 0.1038, Acc: 0.75 \n",
            "LR: [2.0]\n",
            "Epoch [23/70], Loss: 0.0998, Acc: 0.76 \n",
            "LR: [2.0]\n",
            "Epoch [24/70], Loss: 0.1047, Acc: 0.77 \n",
            "LR: [2.0]\n",
            "Epoch [25/70], Loss: 0.1147, Acc: 0.76 \n",
            "LR: [2.0]\n",
            "Epoch [26/70], Loss: 0.1106, Acc: 0.77 \n",
            "LR: [2.0]\n",
            "Epoch [27/70], Loss: 0.0843, Acc: 0.78 \n",
            "LR: [2.0]\n",
            "Epoch [28/70], Loss: 0.1089, Acc: 0.78 \n",
            "LR: [2.0]\n",
            "Epoch [29/70], Loss: 0.0990, Acc: 0.79 \n",
            "LR: [2.0]\n",
            "Epoch [30/70], Loss: 0.1022, Acc: 0.79 \n",
            "LR: [2.0]\n",
            "Epoch [31/70], Loss: 0.0992, Acc: 0.79 \n",
            "LR: [2.0]\n",
            "Epoch [32/70], Loss: 0.0887, Acc: 0.80 \n",
            "LR: [2.0]\n",
            "Epoch [33/70], Loss: 0.0996, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [34/70], Loss: 0.0968, Acc: 0.80 \n",
            "LR: [2.0]\n",
            "Epoch [35/70], Loss: 0.0870, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [36/70], Loss: 0.0866, Acc: 0.82 \n",
            "LR: [2.0]\n",
            "Epoch [37/70], Loss: 0.0845, Acc: 0.82 \n",
            "LR: [2.0]\n",
            "Epoch [38/70], Loss: 0.0877, Acc: 0.82 \n",
            "LR: [2.0]\n",
            "Epoch [39/70], Loss: 0.0966, Acc: 0.82 \n",
            "LR: [2.0]\n",
            "Epoch [40/70], Loss: 0.0873, Acc: 0.82 \n",
            "LR: [2.0]\n",
            "Epoch [41/70], Loss: 0.0896, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [42/70], Loss: 0.0956, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [43/70], Loss: 0.0975, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [44/70], Loss: 0.0899, Acc: 0.84 \n",
            "LR: [2.0]\n",
            "Epoch [45/70], Loss: 0.0929, Acc: 0.84 \n",
            "LR: [2.0]\n",
            "Epoch [46/70], Loss: 0.0960, Acc: 0.84 \n",
            "LR: [2.0]\n",
            "Epoch [47/70], Loss: 0.0852, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [48/70], Loss: 0.0988, Acc: 0.84 \n",
            "LR: [0.4]\n",
            "Epoch [49/70], Loss: 0.1021, Acc: 0.84 \n",
            "LR: [0.4]\n",
            "Epoch [50/70], Loss: 0.0799, Acc: 0.88 \n",
            "LR: [0.4]\n",
            "Epoch [51/70], Loss: 0.0723, Acc: 0.90 \n",
            "LR: [0.4]\n",
            "Epoch [52/70], Loss: 0.0804, Acc: 0.90 \n",
            "LR: [0.4]\n",
            "Epoch [53/70], Loss: 0.0771, Acc: 0.91 \n",
            "LR: [0.4]\n",
            "Epoch [54/70], Loss: 0.0751, Acc: 0.91 \n",
            "LR: [0.4]\n",
            "Epoch [55/70], Loss: 0.0710, Acc: 0.91 \n",
            "LR: [0.4]\n",
            "Epoch [56/70], Loss: 0.0872, Acc: 0.91 \n",
            "LR: [0.4]\n",
            "Epoch [57/70], Loss: 0.0795, Acc: 0.92 \n",
            "LR: [0.4]\n",
            "Epoch [58/70], Loss: 0.0677, Acc: 0.92 \n",
            "LR: [0.4]\n",
            "Epoch [59/70], Loss: 0.0710, Acc: 0.92 \n",
            "LR: [0.4]\n",
            "Epoch [60/70], Loss: 0.0756, Acc: 0.92 \n",
            "LR: [0.4]\n",
            "Epoch [61/70], Loss: 0.0697, Acc: 0.92 \n",
            "LR: [0.4]\n",
            "Epoch [62/70], Loss: 0.0741, Acc: 0.92 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [63/70], Loss: 0.0791, Acc: 0.93 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [64/70], Loss: 0.0690, Acc: 0.93 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [65/70], Loss: 0.0677, Acc: 0.93 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [66/70], Loss: 0.0753, Acc: 0.93 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [67/70], Loss: 0.0793, Acc: 0.93 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [68/70], Loss: 0.0752, Acc: 0.93 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [69/70], Loss: 0.0742, Acc: 0.93 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [70/70], Loss: 0.0743, Acc: 0.93 \n",
            "Constructing exemplar set for class-20...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:172: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done\n",
            "Constructing exemplar set for class-21...\n",
            "Done\n",
            "Constructing exemplar set for class-22...\n",
            "Done\n",
            "Constructing exemplar set for class-23...\n",
            "Done\n",
            "Constructing exemplar set for class-24...\n",
            "Done\n",
            "Constructing exemplar set for class-25...\n",
            "Done\n",
            "Constructing exemplar set for class-26...\n",
            "Done\n",
            "Constructing exemplar set for class-27...\n",
            "Done\n",
            "Constructing exemplar set for class-28...\n",
            "Done\n",
            "Constructing exemplar set for class-29...\n",
            "Done\n",
            "iCaRL classes: 30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:62: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.656333 %\n",
            "0.666\n",
            "6980\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "#####################\n",
            "10 new classes\n",
            "LR: [2.0]\n",
            "Epoch [1/70], Loss: 0.1366, Acc: 0.30 \n",
            "LR: [2.0]\n",
            "Epoch [2/70], Loss: 0.1072, Acc: 0.40 \n",
            "LR: [2.0]\n",
            "Epoch [3/70], Loss: 0.1095, Acc: 0.46 \n",
            "LR: [2.0]\n",
            "Epoch [4/70], Loss: 0.1238, Acc: 0.51 \n",
            "LR: [2.0]\n",
            "Epoch [5/70], Loss: 0.1080, Acc: 0.55 \n",
            "LR: [2.0]\n",
            "Epoch [6/70], Loss: 0.1185, Acc: 0.57 \n",
            "LR: [2.0]\n",
            "Epoch [7/70], Loss: 0.1209, Acc: 0.59 \n",
            "LR: [2.0]\n",
            "Epoch [8/70], Loss: 0.1066, Acc: 0.61 \n",
            "LR: [2.0]\n",
            "Epoch [9/70], Loss: 0.1162, Acc: 0.63 \n",
            "LR: [2.0]\n",
            "Epoch [10/70], Loss: 0.1036, Acc: 0.65 \n",
            "LR: [2.0]\n",
            "Epoch [11/70], Loss: 0.1074, Acc: 0.65 \n",
            "LR: [2.0]\n",
            "Epoch [12/70], Loss: 0.0932, Acc: 0.68 \n",
            "LR: [2.0]\n",
            "Epoch [13/70], Loss: 0.0993, Acc: 0.69 \n",
            "LR: [2.0]\n",
            "Epoch [14/70], Loss: 0.1039, Acc: 0.70 \n",
            "LR: [2.0]\n",
            "Epoch [15/70], Loss: 0.0950, Acc: 0.71 \n",
            "LR: [2.0]\n",
            "Epoch [16/70], Loss: 0.1122, Acc: 0.72 \n",
            "LR: [2.0]\n",
            "Epoch [17/70], Loss: 0.0979, Acc: 0.73 \n",
            "LR: [2.0]\n",
            "Epoch [18/70], Loss: 0.0962, Acc: 0.73 \n",
            "LR: [2.0]\n",
            "Epoch [19/70], Loss: 0.0969, Acc: 0.74 \n",
            "LR: [2.0]\n",
            "Epoch [20/70], Loss: 0.0866, Acc: 0.75 \n",
            "LR: [2.0]\n",
            "Epoch [21/70], Loss: 0.0987, Acc: 0.76 \n",
            "LR: [2.0]\n",
            "Epoch [22/70], Loss: 0.1030, Acc: 0.76 \n",
            "LR: [2.0]\n",
            "Epoch [23/70], Loss: 0.0923, Acc: 0.77 \n",
            "LR: [2.0]\n",
            "Epoch [24/70], Loss: 0.0947, Acc: 0.78 \n",
            "LR: [2.0]\n",
            "Epoch [25/70], Loss: 0.0941, Acc: 0.78 \n",
            "LR: [2.0]\n",
            "Epoch [26/70], Loss: 0.0903, Acc: 0.79 \n",
            "LR: [2.0]\n",
            "Epoch [27/70], Loss: 0.0916, Acc: 0.79 \n",
            "LR: [2.0]\n",
            "Epoch [28/70], Loss: 0.0874, Acc: 0.79 \n",
            "LR: [2.0]\n",
            "Epoch [29/70], Loss: 0.1008, Acc: 0.80 \n",
            "LR: [2.0]\n",
            "Epoch [30/70], Loss: 0.1046, Acc: 0.80 \n",
            "LR: [2.0]\n",
            "Epoch [31/70], Loss: 0.0887, Acc: 0.80 \n",
            "LR: [2.0]\n",
            "Epoch [32/70], Loss: 0.0799, Acc: 0.80 \n",
            "LR: [2.0]\n",
            "Epoch [33/70], Loss: 0.0959, Acc: 0.80 \n",
            "LR: [2.0]\n",
            "Epoch [34/70], Loss: 0.0828, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [35/70], Loss: 0.1005, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [36/70], Loss: 0.0884, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [37/70], Loss: 0.0956, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [38/70], Loss: 0.0892, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [39/70], Loss: 0.0878, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [40/70], Loss: 0.0888, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [41/70], Loss: 0.0858, Acc: 0.82 \n",
            "LR: [2.0]\n",
            "Epoch [42/70], Loss: 0.0938, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [43/70], Loss: 0.1033, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [44/70], Loss: 0.0890, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [45/70], Loss: 0.0924, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [46/70], Loss: 0.0920, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [47/70], Loss: 0.0911, Acc: 0.84 \n",
            "LR: [2.0]\n",
            "Epoch [48/70], Loss: 0.0946, Acc: 0.85 \n",
            "LR: [0.4]\n",
            "Epoch [49/70], Loss: 0.0837, Acc: 0.84 \n",
            "LR: [0.4]\n",
            "Epoch [50/70], Loss: 0.0830, Acc: 0.87 \n",
            "LR: [0.4]\n",
            "Epoch [51/70], Loss: 0.0782, Acc: 0.89 \n",
            "LR: [0.4]\n",
            "Epoch [52/70], Loss: 0.0782, Acc: 0.90 \n",
            "LR: [0.4]\n",
            "Epoch [53/70], Loss: 0.0786, Acc: 0.89 \n",
            "LR: [0.4]\n",
            "Epoch [54/70], Loss: 0.0746, Acc: 0.89 \n",
            "LR: [0.4]\n",
            "Epoch [55/70], Loss: 0.0760, Acc: 0.90 \n",
            "LR: [0.4]\n",
            "Epoch [56/70], Loss: 0.0799, Acc: 0.90 \n",
            "LR: [0.4]\n",
            "Epoch [57/70], Loss: 0.0771, Acc: 0.90 \n",
            "LR: [0.4]\n",
            "Epoch [58/70], Loss: 0.0785, Acc: 0.90 \n",
            "LR: [0.4]\n",
            "Epoch [59/70], Loss: 0.0716, Acc: 0.91 \n",
            "LR: [0.4]\n",
            "Epoch [60/70], Loss: 0.0708, Acc: 0.91 \n",
            "LR: [0.4]\n",
            "Epoch [61/70], Loss: 0.0824, Acc: 0.91 \n",
            "LR: [0.4]\n",
            "Epoch [62/70], Loss: 0.0819, Acc: 0.91 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [63/70], Loss: 0.0803, Acc: 0.90 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [64/70], Loss: 0.0799, Acc: 0.91 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [65/70], Loss: 0.0774, Acc: 0.91 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [66/70], Loss: 0.0790, Acc: 0.91 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [67/70], Loss: 0.0793, Acc: 0.91 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [68/70], Loss: 0.0738, Acc: 0.91 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [69/70], Loss: 0.0825, Acc: 0.91 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [70/70], Loss: 0.0750, Acc: 0.91 \n",
            "Constructing exemplar set for class-30...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:172: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done\n",
            "Constructing exemplar set for class-31...\n",
            "Done\n",
            "Constructing exemplar set for class-32...\n",
            "Done\n",
            "Constructing exemplar set for class-33...\n",
            "Done\n",
            "Constructing exemplar set for class-34...\n",
            "Done\n",
            "Constructing exemplar set for class-35...\n",
            "Done\n",
            "Constructing exemplar set for class-36...\n",
            "Done\n",
            "Constructing exemplar set for class-37...\n",
            "Done\n",
            "Constructing exemplar set for class-38...\n",
            "Done\n",
            "Constructing exemplar set for class-39...\n",
            "Done\n",
            "iCaRL classes: 40\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:62: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.610000 %\n",
            "0.6225\n",
            "7000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "#####################\n",
            "10 new classes\n",
            "LR: [2.0]\n",
            "Epoch [1/70], Loss: 0.1204, Acc: 0.32 \n",
            "LR: [2.0]\n",
            "Epoch [2/70], Loss: 0.0990, Acc: 0.43 \n",
            "LR: [2.0]\n",
            "Epoch [3/70], Loss: 0.1148, Acc: 0.50 \n",
            "LR: [2.0]\n",
            "Epoch [4/70], Loss: 0.1135, Acc: 0.55 \n",
            "LR: [2.0]\n",
            "Epoch [5/70], Loss: 0.1038, Acc: 0.58 \n",
            "LR: [2.0]\n",
            "Epoch [6/70], Loss: 0.0979, Acc: 0.61 \n",
            "LR: [2.0]\n",
            "Epoch [7/70], Loss: 0.0950, Acc: 0.63 \n",
            "LR: [2.0]\n",
            "Epoch [8/70], Loss: 0.1028, Acc: 0.65 \n",
            "LR: [2.0]\n",
            "Epoch [9/70], Loss: 0.1011, Acc: 0.66 \n",
            "LR: [2.0]\n",
            "Epoch [10/70], Loss: 0.1079, Acc: 0.68 \n",
            "LR: [2.0]\n",
            "Epoch [11/70], Loss: 0.1030, Acc: 0.69 \n",
            "LR: [2.0]\n",
            "Epoch [12/70], Loss: 0.0926, Acc: 0.70 \n",
            "LR: [2.0]\n",
            "Epoch [13/70], Loss: 0.1074, Acc: 0.71 \n",
            "LR: [2.0]\n",
            "Epoch [14/70], Loss: 0.0915, Acc: 0.73 \n",
            "LR: [2.0]\n",
            "Epoch [15/70], Loss: 0.0980, Acc: 0.74 \n",
            "LR: [2.0]\n",
            "Epoch [16/70], Loss: 0.0967, Acc: 0.74 \n",
            "LR: [2.0]\n",
            "Epoch [17/70], Loss: 0.0989, Acc: 0.75 \n",
            "LR: [2.0]\n",
            "Epoch [18/70], Loss: 0.1042, Acc: 0.76 \n",
            "LR: [2.0]\n",
            "Epoch [19/70], Loss: 0.0918, Acc: 0.76 \n",
            "LR: [2.0]\n",
            "Epoch [20/70], Loss: 0.0984, Acc: 0.77 \n",
            "LR: [2.0]\n",
            "Epoch [21/70], Loss: 0.0973, Acc: 0.79 \n",
            "LR: [2.0]\n",
            "Epoch [22/70], Loss: 0.0906, Acc: 0.78 \n",
            "LR: [2.0]\n",
            "Epoch [23/70], Loss: 0.0930, Acc: 0.77 \n",
            "LR: [2.0]\n",
            "Epoch [24/70], Loss: 0.0946, Acc: 0.78 \n",
            "LR: [2.0]\n",
            "Epoch [25/70], Loss: 0.0909, Acc: 0.80 \n",
            "LR: [2.0]\n",
            "Epoch [26/70], Loss: 0.0929, Acc: 0.80 \n",
            "LR: [2.0]\n",
            "Epoch [27/70], Loss: 0.0922, Acc: 0.80 \n",
            "LR: [2.0]\n",
            "Epoch [28/70], Loss: 0.0932, Acc: 0.80 \n",
            "LR: [2.0]\n",
            "Epoch [29/70], Loss: 0.0936, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [30/70], Loss: 0.1004, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [31/70], Loss: 0.0989, Acc: 0.82 \n",
            "LR: [2.0]\n",
            "Epoch [32/70], Loss: 0.0913, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [33/70], Loss: 0.1012, Acc: 0.82 \n",
            "LR: [2.0]\n",
            "Epoch [34/70], Loss: 0.0920, Acc: 0.82 \n",
            "LR: [2.0]\n",
            "Epoch [35/70], Loss: 0.0879, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [36/70], Loss: 0.0968, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [37/70], Loss: 0.0942, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [38/70], Loss: 0.0955, Acc: 0.84 \n",
            "LR: [2.0]\n",
            "Epoch [39/70], Loss: 0.0874, Acc: 0.84 \n",
            "LR: [2.0]\n",
            "Epoch [40/70], Loss: 0.0917, Acc: 0.84 \n",
            "LR: [2.0]\n",
            "Epoch [41/70], Loss: 0.0928, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [42/70], Loss: 0.0936, Acc: 0.84 \n",
            "LR: [2.0]\n",
            "Epoch [43/70], Loss: 0.0942, Acc: 0.86 \n",
            "LR: [2.0]\n",
            "Epoch [44/70], Loss: 0.0892, Acc: 0.84 \n",
            "LR: [2.0]\n",
            "Epoch [45/70], Loss: 0.0927, Acc: 0.84 \n",
            "LR: [2.0]\n",
            "Epoch [46/70], Loss: 0.0894, Acc: 0.85 \n",
            "LR: [2.0]\n",
            "Epoch [47/70], Loss: 0.0859, Acc: 0.86 \n",
            "LR: [2.0]\n",
            "Epoch [48/70], Loss: 0.0852, Acc: 0.84 \n",
            "LR: [0.4]\n",
            "Epoch [49/70], Loss: 0.0980, Acc: 0.84 \n",
            "LR: [0.4]\n",
            "Epoch [50/70], Loss: 0.0871, Acc: 0.89 \n",
            "LR: [0.4]\n",
            "Epoch [51/70], Loss: 0.0807, Acc: 0.91 \n",
            "LR: [0.4]\n",
            "Epoch [52/70], Loss: 0.0877, Acc: 0.91 \n",
            "LR: [0.4]\n",
            "Epoch [53/70], Loss: 0.0785, Acc: 0.91 \n",
            "LR: [0.4]\n",
            "Epoch [54/70], Loss: 0.0854, Acc: 0.91 \n",
            "LR: [0.4]\n",
            "Epoch [55/70], Loss: 0.0891, Acc: 0.92 \n",
            "LR: [0.4]\n",
            "Epoch [56/70], Loss: 0.0847, Acc: 0.92 \n",
            "LR: [0.4]\n",
            "Epoch [57/70], Loss: 0.0832, Acc: 0.92 \n",
            "LR: [0.4]\n",
            "Epoch [58/70], Loss: 0.0811, Acc: 0.92 \n",
            "LR: [0.4]\n",
            "Epoch [59/70], Loss: 0.0775, Acc: 0.92 \n",
            "LR: [0.4]\n",
            "Epoch [60/70], Loss: 0.0789, Acc: 0.93 \n",
            "LR: [0.4]\n",
            "Epoch [61/70], Loss: 0.0796, Acc: 0.92 \n",
            "LR: [0.4]\n",
            "Epoch [62/70], Loss: 0.0809, Acc: 0.93 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [63/70], Loss: 0.0839, Acc: 0.93 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [64/70], Loss: 0.0892, Acc: 0.93 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [65/70], Loss: 0.0788, Acc: 0.93 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [66/70], Loss: 0.0843, Acc: 0.93 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [67/70], Loss: 0.0820, Acc: 0.93 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [68/70], Loss: 0.0832, Acc: 0.93 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [69/70], Loss: 0.0826, Acc: 0.93 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [70/70], Loss: 0.0810, Acc: 0.93 \n",
            "Constructing exemplar set for class-40...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:172: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done\n",
            "Constructing exemplar set for class-41...\n",
            "Done\n",
            "Constructing exemplar set for class-42...\n",
            "Done\n",
            "Constructing exemplar set for class-43...\n",
            "Done\n",
            "Constructing exemplar set for class-44...\n",
            "Done\n",
            "Constructing exemplar set for class-45...\n",
            "Done\n",
            "Constructing exemplar set for class-46...\n",
            "Done\n",
            "Constructing exemplar set for class-47...\n",
            "Done\n",
            "Constructing exemplar set for class-48...\n",
            "Done\n",
            "Constructing exemplar set for class-49...\n",
            "Done\n",
            "iCaRL classes: 50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:62: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.553600 %\n",
            "0.5576\n",
            "7000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "#####################\n",
            "10 new classes\n",
            "LR: [2.0]\n",
            "Epoch [1/70], Loss: 0.1197, Acc: 0.34 \n",
            "LR: [2.0]\n",
            "Epoch [2/70], Loss: 0.1049, Acc: 0.50 \n",
            "LR: [2.0]\n",
            "Epoch [3/70], Loss: 0.1082, Acc: 0.57 \n",
            "LR: [2.0]\n",
            "Epoch [4/70], Loss: 0.0958, Acc: 0.61 \n",
            "LR: [2.0]\n",
            "Epoch [5/70], Loss: 0.0982, Acc: 0.64 \n",
            "LR: [2.0]\n",
            "Epoch [6/70], Loss: 0.1008, Acc: 0.65 \n",
            "LR: [2.0]\n",
            "Epoch [7/70], Loss: 0.1084, Acc: 0.67 \n",
            "LR: [2.0]\n",
            "Epoch [8/70], Loss: 0.0998, Acc: 0.70 \n",
            "LR: [2.0]\n",
            "Epoch [9/70], Loss: 0.1046, Acc: 0.72 \n",
            "LR: [2.0]\n",
            "Epoch [10/70], Loss: 0.0932, Acc: 0.73 \n",
            "LR: [2.0]\n",
            "Epoch [11/70], Loss: 0.0983, Acc: 0.74 \n",
            "LR: [2.0]\n",
            "Epoch [12/70], Loss: 0.0941, Acc: 0.74 \n",
            "LR: [2.0]\n",
            "Epoch [13/70], Loss: 0.0891, Acc: 0.76 \n",
            "LR: [2.0]\n",
            "Epoch [14/70], Loss: 0.1016, Acc: 0.76 \n",
            "LR: [2.0]\n",
            "Epoch [15/70], Loss: 0.0890, Acc: 0.78 \n",
            "LR: [2.0]\n",
            "Epoch [16/70], Loss: 0.0882, Acc: 0.78 \n",
            "LR: [2.0]\n",
            "Epoch [17/70], Loss: 0.0932, Acc: 0.79 \n",
            "LR: [2.0]\n",
            "Epoch [18/70], Loss: 0.0924, Acc: 0.79 \n",
            "LR: [2.0]\n",
            "Epoch [19/70], Loss: 0.0949, Acc: 0.80 \n",
            "LR: [2.0]\n",
            "Epoch [20/70], Loss: 0.0916, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [21/70], Loss: 0.0927, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [22/70], Loss: 0.0934, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [23/70], Loss: 0.0927, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [24/70], Loss: 0.0943, Acc: 0.82 \n",
            "LR: [2.0]\n",
            "Epoch [25/70], Loss: 0.0884, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [26/70], Loss: 0.0856, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [27/70], Loss: 0.0876, Acc: 0.84 \n",
            "LR: [2.0]\n",
            "Epoch [28/70], Loss: 0.0950, Acc: 0.84 \n",
            "LR: [2.0]\n",
            "Epoch [29/70], Loss: 0.0912, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [30/70], Loss: 0.0964, Acc: 0.84 \n",
            "LR: [2.0]\n",
            "Epoch [31/70], Loss: 0.0988, Acc: 0.84 \n",
            "LR: [2.0]\n",
            "Epoch [32/70], Loss: 0.0923, Acc: 0.84 \n",
            "LR: [2.0]\n",
            "Epoch [33/70], Loss: 0.0922, Acc: 0.84 \n",
            "LR: [2.0]\n",
            "Epoch [34/70], Loss: 0.0908, Acc: 0.84 \n",
            "LR: [2.0]\n",
            "Epoch [35/70], Loss: 0.0959, Acc: 0.85 \n",
            "LR: [2.0]\n",
            "Epoch [36/70], Loss: 0.0861, Acc: 0.85 \n",
            "LR: [2.0]\n",
            "Epoch [37/70], Loss: 0.0873, Acc: 0.85 \n",
            "LR: [2.0]\n",
            "Epoch [38/70], Loss: 0.0932, Acc: 0.85 \n",
            "LR: [2.0]\n",
            "Epoch [39/70], Loss: 0.0938, Acc: 0.85 \n",
            "LR: [2.0]\n",
            "Epoch [40/70], Loss: 0.0866, Acc: 0.86 \n",
            "LR: [2.0]\n",
            "Epoch [41/70], Loss: 0.0911, Acc: 0.86 \n",
            "LR: [2.0]\n",
            "Epoch [42/70], Loss: 0.0945, Acc: 0.86 \n",
            "LR: [2.0]\n",
            "Epoch [43/70], Loss: 0.0899, Acc: 0.86 \n",
            "LR: [2.0]\n",
            "Epoch [44/70], Loss: 0.0895, Acc: 0.86 \n",
            "LR: [2.0]\n",
            "Epoch [45/70], Loss: 0.0877, Acc: 0.86 \n",
            "LR: [2.0]\n",
            "Epoch [46/70], Loss: 0.0906, Acc: 0.86 \n",
            "LR: [2.0]\n",
            "Epoch [47/70], Loss: 0.0843, Acc: 0.85 \n",
            "LR: [2.0]\n",
            "Epoch [48/70], Loss: 0.0875, Acc: 0.86 \n",
            "LR: [0.4]\n",
            "Epoch [49/70], Loss: 0.0908, Acc: 0.86 \n",
            "LR: [0.4]\n",
            "Epoch [50/70], Loss: 0.0850, Acc: 0.89 \n",
            "LR: [0.4]\n",
            "Epoch [51/70], Loss: 0.0902, Acc: 0.91 \n",
            "LR: [0.4]\n",
            "Epoch [52/70], Loss: 0.0815, Acc: 0.91 \n",
            "LR: [0.4]\n",
            "Epoch [53/70], Loss: 0.0829, Acc: 0.92 \n",
            "LR: [0.4]\n",
            "Epoch [54/70], Loss: 0.0794, Acc: 0.91 \n",
            "LR: [0.4]\n",
            "Epoch [55/70], Loss: 0.0861, Acc: 0.92 \n",
            "LR: [0.4]\n",
            "Epoch [56/70], Loss: 0.0825, Acc: 0.92 \n",
            "LR: [0.4]\n",
            "Epoch [57/70], Loss: 0.0858, Acc: 0.92 \n",
            "LR: [0.4]\n",
            "Epoch [58/70], Loss: 0.0844, Acc: 0.92 \n",
            "LR: [0.4]\n",
            "Epoch [59/70], Loss: 0.0789, Acc: 0.92 \n",
            "LR: [0.4]\n",
            "Epoch [60/70], Loss: 0.0886, Acc: 0.92 \n",
            "LR: [0.4]\n",
            "Epoch [61/70], Loss: 0.0839, Acc: 0.92 \n",
            "LR: [0.4]\n",
            "Epoch [62/70], Loss: 0.0852, Acc: 0.92 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [63/70], Loss: 0.0827, Acc: 0.93 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [64/70], Loss: 0.0863, Acc: 0.93 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [65/70], Loss: 0.0863, Acc: 0.92 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [66/70], Loss: 0.0810, Acc: 0.93 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [67/70], Loss: 0.0806, Acc: 0.93 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [68/70], Loss: 0.0836, Acc: 0.92 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [69/70], Loss: 0.0826, Acc: 0.92 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [70/70], Loss: 0.0790, Acc: 0.93 \n",
            "Constructing exemplar set for class-50...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:172: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done\n",
            "Constructing exemplar set for class-51...\n",
            "Done\n",
            "Constructing exemplar set for class-52...\n",
            "Done\n",
            "Constructing exemplar set for class-53...\n",
            "Done\n",
            "Constructing exemplar set for class-54...\n",
            "Done\n",
            "Constructing exemplar set for class-55...\n",
            "Done\n",
            "Constructing exemplar set for class-56...\n",
            "Done\n",
            "Constructing exemplar set for class-57...\n",
            "Done\n",
            "Constructing exemplar set for class-58...\n",
            "Done\n",
            "Constructing exemplar set for class-59...\n",
            "Done\n",
            "iCaRL classes: 60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:62: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.515667 %\n",
            "0.5215\n",
            "6980\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "#####################\n",
            "10 new classes\n",
            "LR: [2.0]\n",
            "Epoch [1/70], Loss: 0.1139, Acc: 0.29 \n",
            "LR: [2.0]\n",
            "Epoch [2/70], Loss: 0.1111, Acc: 0.37 \n",
            "LR: [2.0]\n",
            "Epoch [3/70], Loss: 0.1042, Acc: 0.43 \n",
            "LR: [2.0]\n",
            "Epoch [4/70], Loss: 0.1163, Acc: 0.46 \n",
            "LR: [2.0]\n",
            "Epoch [5/70], Loss: 0.0998, Acc: 0.49 \n",
            "LR: [2.0]\n",
            "Epoch [6/70], Loss: 0.1036, Acc: 0.52 \n",
            "LR: [2.0]\n",
            "Epoch [7/70], Loss: 0.1149, Acc: 0.54 \n",
            "LR: [2.0]\n",
            "Epoch [8/70], Loss: 0.1016, Acc: 0.56 \n",
            "LR: [2.0]\n",
            "Epoch [9/70], Loss: 0.1104, Acc: 0.58 \n",
            "LR: [2.0]\n",
            "Epoch [10/70], Loss: 0.1055, Acc: 0.59 \n",
            "LR: [2.0]\n",
            "Epoch [11/70], Loss: 0.1092, Acc: 0.61 \n",
            "LR: [2.0]\n",
            "Epoch [12/70], Loss: 0.1069, Acc: 0.63 \n",
            "LR: [2.0]\n",
            "Epoch [13/70], Loss: 0.1009, Acc: 0.64 \n",
            "LR: [2.0]\n",
            "Epoch [14/70], Loss: 0.1054, Acc: 0.64 \n",
            "LR: [2.0]\n",
            "Epoch [15/70], Loss: 0.1054, Acc: 0.66 \n",
            "LR: [2.0]\n",
            "Epoch [16/70], Loss: 0.1013, Acc: 0.66 \n",
            "LR: [2.0]\n",
            "Epoch [17/70], Loss: 0.0991, Acc: 0.68 \n",
            "LR: [2.0]\n",
            "Epoch [18/70], Loss: 0.1080, Acc: 0.68 \n",
            "LR: [2.0]\n",
            "Epoch [19/70], Loss: 0.1002, Acc: 0.69 \n",
            "LR: [2.0]\n",
            "Epoch [20/70], Loss: 0.0987, Acc: 0.70 \n",
            "LR: [2.0]\n",
            "Epoch [21/70], Loss: 0.1036, Acc: 0.71 \n",
            "LR: [2.0]\n",
            "Epoch [22/70], Loss: 0.0994, Acc: 0.72 \n",
            "LR: [2.0]\n",
            "Epoch [23/70], Loss: 0.0957, Acc: 0.73 \n",
            "LR: [2.0]\n",
            "Epoch [24/70], Loss: 0.0929, Acc: 0.73 \n",
            "LR: [2.0]\n",
            "Epoch [25/70], Loss: 0.1095, Acc: 0.73 \n",
            "LR: [2.0]\n",
            "Epoch [26/70], Loss: 0.0996, Acc: 0.74 \n",
            "LR: [2.0]\n",
            "Epoch [27/70], Loss: 0.0977, Acc: 0.75 \n",
            "LR: [2.0]\n",
            "Epoch [28/70], Loss: 0.0931, Acc: 0.76 \n",
            "LR: [2.0]\n",
            "Epoch [29/70], Loss: 0.0995, Acc: 0.77 \n",
            "LR: [2.0]\n",
            "Epoch [30/70], Loss: 0.1088, Acc: 0.76 \n",
            "LR: [2.0]\n",
            "Epoch [31/70], Loss: 0.1149, Acc: 0.76 \n",
            "LR: [2.0]\n",
            "Epoch [32/70], Loss: 0.0981, Acc: 0.76 \n",
            "LR: [2.0]\n",
            "Epoch [33/70], Loss: 0.0975, Acc: 0.77 \n",
            "LR: [2.0]\n",
            "Epoch [34/70], Loss: 0.0978, Acc: 0.78 \n",
            "LR: [2.0]\n",
            "Epoch [35/70], Loss: 0.0946, Acc: 0.78 \n",
            "LR: [2.0]\n",
            "Epoch [36/70], Loss: 0.1027, Acc: 0.79 \n",
            "LR: [2.0]\n",
            "Epoch [37/70], Loss: 0.1019, Acc: 0.79 \n",
            "LR: [2.0]\n",
            "Epoch [38/70], Loss: 0.1010, Acc: 0.78 \n",
            "LR: [2.0]\n",
            "Epoch [39/70], Loss: 0.0963, Acc: 0.79 \n",
            "LR: [2.0]\n",
            "Epoch [40/70], Loss: 0.0980, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [41/70], Loss: 0.0982, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [42/70], Loss: 0.0986, Acc: 0.80 \n",
            "LR: [2.0]\n",
            "Epoch [43/70], Loss: 0.1016, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [44/70], Loss: 0.0977, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [45/70], Loss: 0.0998, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [46/70], Loss: 0.0907, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [47/70], Loss: 0.0976, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [48/70], Loss: 0.0986, Acc: 0.81 \n",
            "LR: [0.4]\n",
            "Epoch [49/70], Loss: 0.0886, Acc: 0.82 \n",
            "LR: [0.4]\n",
            "Epoch [50/70], Loss: 0.0962, Acc: 0.87 \n",
            "LR: [0.4]\n",
            "Epoch [51/70], Loss: 0.0929, Acc: 0.89 \n",
            "LR: [0.4]\n",
            "Epoch [52/70], Loss: 0.0927, Acc: 0.89 \n",
            "LR: [0.4]\n",
            "Epoch [53/70], Loss: 0.0931, Acc: 0.90 \n",
            "LR: [0.4]\n",
            "Epoch [54/70], Loss: 0.0854, Acc: 0.90 \n",
            "LR: [0.4]\n",
            "Epoch [55/70], Loss: 0.0875, Acc: 0.90 \n",
            "LR: [0.4]\n",
            "Epoch [56/70], Loss: 0.0947, Acc: 0.90 \n",
            "LR: [0.4]\n",
            "Epoch [57/70], Loss: 0.0855, Acc: 0.90 \n",
            "LR: [0.4]\n",
            "Epoch [58/70], Loss: 0.0902, Acc: 0.91 \n",
            "LR: [0.4]\n",
            "Epoch [59/70], Loss: 0.0838, Acc: 0.91 \n",
            "LR: [0.4]\n",
            "Epoch [60/70], Loss: 0.0950, Acc: 0.91 \n",
            "LR: [0.4]\n",
            "Epoch [61/70], Loss: 0.0940, Acc: 0.91 \n",
            "LR: [0.4]\n",
            "Epoch [62/70], Loss: 0.0858, Acc: 0.91 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [63/70], Loss: 0.0909, Acc: 0.91 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [64/70], Loss: 0.0934, Acc: 0.91 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [65/70], Loss: 0.0927, Acc: 0.91 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [66/70], Loss: 0.0920, Acc: 0.92 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [67/70], Loss: 0.0916, Acc: 0.92 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [68/70], Loss: 0.0884, Acc: 0.92 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [69/70], Loss: 0.0872, Acc: 0.92 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [70/70], Loss: 0.0917, Acc: 0.92 \n",
            "Constructing exemplar set for class-60...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:172: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done\n",
            "Constructing exemplar set for class-61...\n",
            "Done\n",
            "Constructing exemplar set for class-62...\n",
            "Done\n",
            "Constructing exemplar set for class-63...\n",
            "Done\n",
            "Constructing exemplar set for class-64...\n",
            "Done\n",
            "Constructing exemplar set for class-65...\n",
            "Done\n",
            "Constructing exemplar set for class-66...\n",
            "Done\n",
            "Constructing exemplar set for class-67...\n",
            "Done\n",
            "Constructing exemplar set for class-68...\n",
            "Done\n",
            "Constructing exemplar set for class-69...\n",
            "Done\n",
            "iCaRL classes: 70\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:62: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.473143 %\n",
            "0.46214285714285713\n",
            "6960\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "#####################\n",
            "10 new classes\n",
            "LR: [2.0]\n",
            "Epoch [1/70], Loss: 0.1170, Acc: 0.31 \n",
            "LR: [2.0]\n",
            "Epoch [2/70], Loss: 0.1140, Acc: 0.42 \n",
            "LR: [2.0]\n",
            "Epoch [3/70], Loss: 0.0978, Acc: 0.50 \n",
            "LR: [2.0]\n",
            "Epoch [4/70], Loss: 0.0968, Acc: 0.53 \n",
            "LR: [2.0]\n",
            "Epoch [5/70], Loss: 0.1113, Acc: 0.58 \n",
            "LR: [2.0]\n",
            "Epoch [6/70], Loss: 0.1069, Acc: 0.60 \n",
            "LR: [2.0]\n",
            "Epoch [7/70], Loss: 0.1033, Acc: 0.62 \n",
            "LR: [2.0]\n",
            "Epoch [8/70], Loss: 0.1055, Acc: 0.65 \n",
            "LR: [2.0]\n",
            "Epoch [9/70], Loss: 0.0988, Acc: 0.66 \n",
            "LR: [2.0]\n",
            "Epoch [10/70], Loss: 0.1023, Acc: 0.68 \n",
            "LR: [2.0]\n",
            "Epoch [11/70], Loss: 0.0999, Acc: 0.69 \n",
            "LR: [2.0]\n",
            "Epoch [12/70], Loss: 0.1070, Acc: 0.70 \n",
            "LR: [2.0]\n",
            "Epoch [13/70], Loss: 0.0976, Acc: 0.71 \n",
            "LR: [2.0]\n",
            "Epoch [14/70], Loss: 0.0968, Acc: 0.72 \n",
            "LR: [2.0]\n",
            "Epoch [15/70], Loss: 0.0992, Acc: 0.72 \n",
            "LR: [2.0]\n",
            "Epoch [16/70], Loss: 0.1051, Acc: 0.74 \n",
            "LR: [2.0]\n",
            "Epoch [17/70], Loss: 0.1057, Acc: 0.74 \n",
            "LR: [2.0]\n",
            "Epoch [18/70], Loss: 0.0951, Acc: 0.75 \n",
            "LR: [2.0]\n",
            "Epoch [19/70], Loss: 0.0994, Acc: 0.76 \n",
            "LR: [2.0]\n",
            "Epoch [20/70], Loss: 0.0987, Acc: 0.76 \n",
            "LR: [2.0]\n",
            "Epoch [21/70], Loss: 0.0982, Acc: 0.78 \n",
            "LR: [2.0]\n",
            "Epoch [22/70], Loss: 0.1005, Acc: 0.78 \n",
            "LR: [2.0]\n",
            "Epoch [23/70], Loss: 0.0946, Acc: 0.78 \n",
            "LR: [2.0]\n",
            "Epoch [24/70], Loss: 0.1044, Acc: 0.80 \n",
            "LR: [2.0]\n",
            "Epoch [25/70], Loss: 0.0921, Acc: 0.79 \n",
            "LR: [2.0]\n",
            "Epoch [26/70], Loss: 0.0963, Acc: 0.80 \n",
            "LR: [2.0]\n",
            "Epoch [27/70], Loss: 0.0935, Acc: 0.80 \n",
            "LR: [2.0]\n",
            "Epoch [28/70], Loss: 0.0884, Acc: 0.80 \n",
            "LR: [2.0]\n",
            "Epoch [29/70], Loss: 0.0944, Acc: 0.79 \n",
            "LR: [2.0]\n",
            "Epoch [30/70], Loss: 0.1033, Acc: 0.80 \n",
            "LR: [2.0]\n",
            "Epoch [31/70], Loss: 0.1007, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [32/70], Loss: 0.0969, Acc: 0.82 \n",
            "LR: [2.0]\n",
            "Epoch [33/70], Loss: 0.0958, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [34/70], Loss: 0.0981, Acc: 0.82 \n",
            "LR: [2.0]\n",
            "Epoch [35/70], Loss: 0.1041, Acc: 0.82 \n",
            "LR: [2.0]\n",
            "Epoch [36/70], Loss: 0.0957, Acc: 0.82 \n",
            "LR: [2.0]\n",
            "Epoch [37/70], Loss: 0.0987, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [38/70], Loss: 0.0984, Acc: 0.82 \n",
            "LR: [2.0]\n",
            "Epoch [39/70], Loss: 0.1053, Acc: 0.82 \n",
            "LR: [2.0]\n",
            "Epoch [40/70], Loss: 0.0996, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [41/70], Loss: 0.0999, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [42/70], Loss: 0.0917, Acc: 0.82 \n",
            "LR: [2.0]\n",
            "Epoch [43/70], Loss: 0.0966, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [44/70], Loss: 0.1000, Acc: 0.84 \n",
            "LR: [2.0]\n",
            "Epoch [45/70], Loss: 0.1078, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [46/70], Loss: 0.0984, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [47/70], Loss: 0.0929, Acc: 0.84 \n",
            "LR: [2.0]\n",
            "Epoch [48/70], Loss: 0.1018, Acc: 0.85 \n",
            "LR: [0.4]\n",
            "Epoch [49/70], Loss: 0.0940, Acc: 0.84 \n",
            "LR: [0.4]\n",
            "Epoch [50/70], Loss: 0.0893, Acc: 0.87 \n",
            "LR: [0.4]\n",
            "Epoch [51/70], Loss: 0.0841, Acc: 0.89 \n",
            "LR: [0.4]\n",
            "Epoch [52/70], Loss: 0.0998, Acc: 0.90 \n",
            "LR: [0.4]\n",
            "Epoch [53/70], Loss: 0.0903, Acc: 0.90 \n",
            "LR: [0.4]\n",
            "Epoch [54/70], Loss: 0.0871, Acc: 0.90 \n",
            "LR: [0.4]\n",
            "Epoch [55/70], Loss: 0.0892, Acc: 0.90 \n",
            "LR: [0.4]\n",
            "Epoch [56/70], Loss: 0.0954, Acc: 0.90 \n",
            "LR: [0.4]\n",
            "Epoch [57/70], Loss: 0.0953, Acc: 0.91 \n",
            "LR: [0.4]\n",
            "Epoch [58/70], Loss: 0.0914, Acc: 0.91 \n",
            "LR: [0.4]\n",
            "Epoch [59/70], Loss: 0.0845, Acc: 0.90 \n",
            "LR: [0.4]\n",
            "Epoch [60/70], Loss: 0.0941, Acc: 0.90 \n",
            "LR: [0.4]\n",
            "Epoch [61/70], Loss: 0.0871, Acc: 0.91 \n",
            "LR: [0.4]\n",
            "Epoch [62/70], Loss: 0.0920, Acc: 0.91 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [63/70], Loss: 0.0850, Acc: 0.91 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [64/70], Loss: 0.0886, Acc: 0.91 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [65/70], Loss: 0.0911, Acc: 0.92 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [66/70], Loss: 0.0875, Acc: 0.91 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [67/70], Loss: 0.0952, Acc: 0.91 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [68/70], Loss: 0.0931, Acc: 0.91 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [69/70], Loss: 0.0937, Acc: 0.91 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [70/70], Loss: 0.0934, Acc: 0.91 \n",
            "Constructing exemplar set for class-70...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:172: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done\n",
            "Constructing exemplar set for class-71...\n",
            "Done\n",
            "Constructing exemplar set for class-72...\n",
            "Done\n",
            "Constructing exemplar set for class-73...\n",
            "Done\n",
            "Constructing exemplar set for class-74...\n",
            "Done\n",
            "Constructing exemplar set for class-75...\n",
            "Done\n",
            "Constructing exemplar set for class-76...\n",
            "Done\n",
            "Constructing exemplar set for class-77...\n",
            "Done\n",
            "Constructing exemplar set for class-78...\n",
            "Done\n",
            "Constructing exemplar set for class-79...\n",
            "Done\n",
            "iCaRL classes: 80\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:62: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.435750 %\n",
            "0.411\n",
            "7000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "#####################\n",
            "10 new classes\n",
            "LR: [2.0]\n",
            "Epoch [1/70], Loss: 0.1017, Acc: 0.31 \n",
            "LR: [2.0]\n",
            "Epoch [2/70], Loss: 0.1061, Acc: 0.47 \n",
            "LR: [2.0]\n",
            "Epoch [3/70], Loss: 0.1018, Acc: 0.54 \n",
            "LR: [2.0]\n",
            "Epoch [4/70], Loss: 0.0962, Acc: 0.58 \n",
            "LR: [2.0]\n",
            "Epoch [5/70], Loss: 0.1032, Acc: 0.60 \n",
            "LR: [2.0]\n",
            "Epoch [6/70], Loss: 0.0984, Acc: 0.63 \n",
            "LR: [2.0]\n",
            "Epoch [7/70], Loss: 0.0964, Acc: 0.65 \n",
            "LR: [2.0]\n",
            "Epoch [8/70], Loss: 0.0984, Acc: 0.67 \n",
            "LR: [2.0]\n",
            "Epoch [9/70], Loss: 0.0886, Acc: 0.69 \n",
            "LR: [2.0]\n",
            "Epoch [10/70], Loss: 0.0961, Acc: 0.70 \n",
            "LR: [2.0]\n",
            "Epoch [11/70], Loss: 0.0988, Acc: 0.70 \n",
            "LR: [2.0]\n",
            "Epoch [12/70], Loss: 0.0990, Acc: 0.72 \n",
            "LR: [2.0]\n",
            "Epoch [13/70], Loss: 0.0971, Acc: 0.72 \n",
            "LR: [2.0]\n",
            "Epoch [14/70], Loss: 0.0932, Acc: 0.73 \n",
            "LR: [2.0]\n",
            "Epoch [15/70], Loss: 0.0944, Acc: 0.74 \n",
            "LR: [2.0]\n",
            "Epoch [16/70], Loss: 0.0963, Acc: 0.75 \n",
            "LR: [2.0]\n",
            "Epoch [17/70], Loss: 0.1025, Acc: 0.76 \n",
            "LR: [2.0]\n",
            "Epoch [18/70], Loss: 0.0983, Acc: 0.76 \n",
            "LR: [2.0]\n",
            "Epoch [19/70], Loss: 0.0964, Acc: 0.77 \n",
            "LR: [2.0]\n",
            "Epoch [20/70], Loss: 0.0914, Acc: 0.78 \n",
            "LR: [2.0]\n",
            "Epoch [21/70], Loss: 0.0945, Acc: 0.78 \n",
            "LR: [2.0]\n",
            "Epoch [22/70], Loss: 0.0917, Acc: 0.78 \n",
            "LR: [2.0]\n",
            "Epoch [23/70], Loss: 0.0926, Acc: 0.78 \n",
            "LR: [2.0]\n",
            "Epoch [24/70], Loss: 0.0977, Acc: 0.79 \n",
            "LR: [2.0]\n",
            "Epoch [25/70], Loss: 0.0925, Acc: 0.79 \n",
            "LR: [2.0]\n",
            "Epoch [26/70], Loss: 0.0987, Acc: 0.80 \n",
            "LR: [2.0]\n",
            "Epoch [27/70], Loss: 0.0986, Acc: 0.79 \n",
            "LR: [2.0]\n",
            "Epoch [28/70], Loss: 0.0962, Acc: 0.80 \n",
            "LR: [2.0]\n",
            "Epoch [29/70], Loss: 0.0960, Acc: 0.80 \n",
            "LR: [2.0]\n",
            "Epoch [30/70], Loss: 0.0922, Acc: 0.80 \n",
            "LR: [2.0]\n",
            "Epoch [31/70], Loss: 0.0946, Acc: 0.80 \n",
            "LR: [2.0]\n",
            "Epoch [32/70], Loss: 0.0950, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [33/70], Loss: 0.0952, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [34/70], Loss: 0.0936, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [35/70], Loss: 0.0930, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [36/70], Loss: 0.1008, Acc: 0.82 \n",
            "LR: [2.0]\n",
            "Epoch [37/70], Loss: 0.0905, Acc: 0.80 \n",
            "LR: [2.0]\n",
            "Epoch [38/70], Loss: 0.0955, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [39/70], Loss: 0.0983, Acc: 0.82 \n",
            "LR: [2.0]\n",
            "Epoch [40/70], Loss: 0.0979, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [41/70], Loss: 0.0934, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [42/70], Loss: 0.0942, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [43/70], Loss: 0.0913, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [44/70], Loss: 0.0939, Acc: 0.84 \n",
            "LR: [2.0]\n",
            "Epoch [45/70], Loss: 0.0924, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [46/70], Loss: 0.0903, Acc: 0.84 \n",
            "LR: [2.0]\n",
            "Epoch [47/70], Loss: 0.0923, Acc: 0.84 \n",
            "LR: [2.0]\n",
            "Epoch [48/70], Loss: 0.0898, Acc: 0.83 \n",
            "LR: [0.4]\n",
            "Epoch [49/70], Loss: 0.0934, Acc: 0.83 \n",
            "LR: [0.4]\n",
            "Epoch [50/70], Loss: 0.0844, Acc: 0.87 \n",
            "LR: [0.4]\n",
            "Epoch [51/70], Loss: 0.0927, Acc: 0.88 \n",
            "LR: [0.4]\n",
            "Epoch [52/70], Loss: 0.0855, Acc: 0.89 \n",
            "LR: [0.4]\n",
            "Epoch [53/70], Loss: 0.0850, Acc: 0.89 \n",
            "LR: [0.4]\n",
            "Epoch [54/70], Loss: 0.0891, Acc: 0.89 \n",
            "LR: [0.4]\n",
            "Epoch [55/70], Loss: 0.0939, Acc: 0.89 \n",
            "LR: [0.4]\n",
            "Epoch [56/70], Loss: 0.0925, Acc: 0.89 \n",
            "LR: [0.4]\n",
            "Epoch [57/70], Loss: 0.0882, Acc: 0.89 \n",
            "LR: [0.4]\n",
            "Epoch [58/70], Loss: 0.0930, Acc: 0.89 \n",
            "LR: [0.4]\n",
            "Epoch [59/70], Loss: 0.0920, Acc: 0.89 \n",
            "LR: [0.4]\n",
            "Epoch [60/70], Loss: 0.0904, Acc: 0.90 \n",
            "LR: [0.4]\n",
            "Epoch [61/70], Loss: 0.0868, Acc: 0.90 \n",
            "LR: [0.4]\n",
            "Epoch [62/70], Loss: 0.0921, Acc: 0.90 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [63/70], Loss: 0.0860, Acc: 0.90 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [64/70], Loss: 0.0870, Acc: 0.90 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [65/70], Loss: 0.0880, Acc: 0.90 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [66/70], Loss: 0.0880, Acc: 0.90 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [67/70], Loss: 0.0867, Acc: 0.90 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [68/70], Loss: 0.0859, Acc: 0.90 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [69/70], Loss: 0.0855, Acc: 0.90 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [70/70], Loss: 0.0921, Acc: 0.90 \n",
            "Constructing exemplar set for class-80...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:172: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done\n",
            "Constructing exemplar set for class-81...\n",
            "Done\n",
            "Constructing exemplar set for class-82...\n",
            "Done\n",
            "Constructing exemplar set for class-83...\n",
            "Done\n",
            "Constructing exemplar set for class-84...\n",
            "Done\n",
            "Constructing exemplar set for class-85...\n",
            "Done\n",
            "Constructing exemplar set for class-86...\n",
            "Done\n",
            "Constructing exemplar set for class-87...\n",
            "Done\n",
            "Constructing exemplar set for class-88...\n",
            "Done\n",
            "Constructing exemplar set for class-89...\n",
            "Done\n",
            "iCaRL classes: 90\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:62: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.413000 %\n",
            "0.3932222222222222\n",
            "6980\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "#####################\n",
            "10 new classes\n",
            "LR: [2.0]\n",
            "Epoch [1/70], Loss: 0.1088, Acc: 0.30 \n",
            "LR: [2.0]\n",
            "Epoch [2/70], Loss: 0.1021, Acc: 0.46 \n",
            "LR: [2.0]\n",
            "Epoch [3/70], Loss: 0.1001, Acc: 0.54 \n",
            "LR: [2.0]\n",
            "Epoch [4/70], Loss: 0.1024, Acc: 0.58 \n",
            "LR: [2.0]\n",
            "Epoch [5/70], Loss: 0.0947, Acc: 0.62 \n",
            "LR: [2.0]\n",
            "Epoch [6/70], Loss: 0.0960, Acc: 0.64 \n",
            "LR: [2.0]\n",
            "Epoch [7/70], Loss: 0.1029, Acc: 0.66 \n",
            "LR: [2.0]\n",
            "Epoch [8/70], Loss: 0.1025, Acc: 0.68 \n",
            "LR: [2.0]\n",
            "Epoch [9/70], Loss: 0.1019, Acc: 0.69 \n",
            "LR: [2.0]\n",
            "Epoch [10/70], Loss: 0.0968, Acc: 0.71 \n",
            "LR: [2.0]\n",
            "Epoch [11/70], Loss: 0.0928, Acc: 0.72 \n",
            "LR: [2.0]\n",
            "Epoch [12/70], Loss: 0.0977, Acc: 0.72 \n",
            "LR: [2.0]\n",
            "Epoch [13/70], Loss: 0.0960, Acc: 0.74 \n",
            "LR: [2.0]\n",
            "Epoch [14/70], Loss: 0.0956, Acc: 0.74 \n",
            "LR: [2.0]\n",
            "Epoch [15/70], Loss: 0.0924, Acc: 0.75 \n",
            "LR: [2.0]\n",
            "Epoch [16/70], Loss: 0.0956, Acc: 0.77 \n",
            "LR: [2.0]\n",
            "Epoch [17/70], Loss: 0.0971, Acc: 0.76 \n",
            "LR: [2.0]\n",
            "Epoch [18/70], Loss: 0.1057, Acc: 0.77 \n",
            "LR: [2.0]\n",
            "Epoch [19/70], Loss: 0.0941, Acc: 0.77 \n",
            "LR: [2.0]\n",
            "Epoch [20/70], Loss: 0.0982, Acc: 0.77 \n",
            "LR: [2.0]\n",
            "Epoch [21/70], Loss: 0.0938, Acc: 0.78 \n",
            "LR: [2.0]\n",
            "Epoch [22/70], Loss: 0.0954, Acc: 0.79 \n",
            "LR: [2.0]\n",
            "Epoch [23/70], Loss: 0.0929, Acc: 0.80 \n",
            "LR: [2.0]\n",
            "Epoch [24/70], Loss: 0.0960, Acc: 0.79 \n",
            "LR: [2.0]\n",
            "Epoch [25/70], Loss: 0.0957, Acc: 0.80 \n",
            "LR: [2.0]\n",
            "Epoch [26/70], Loss: 0.0973, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [27/70], Loss: 0.0964, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [28/70], Loss: 0.0898, Acc: 0.80 \n",
            "LR: [2.0]\n",
            "Epoch [29/70], Loss: 0.0927, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [30/70], Loss: 0.0962, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [31/70], Loss: 0.0935, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [32/70], Loss: 0.0933, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [33/70], Loss: 0.0965, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [34/70], Loss: 0.0997, Acc: 0.82 \n",
            "LR: [2.0]\n",
            "Epoch [35/70], Loss: 0.0972, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [36/70], Loss: 0.0899, Acc: 0.82 \n",
            "LR: [2.0]\n",
            "Epoch [37/70], Loss: 0.0936, Acc: 0.82 \n",
            "LR: [2.0]\n",
            "Epoch [38/70], Loss: 0.0976, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [39/70], Loss: 0.0965, Acc: 0.81 \n",
            "LR: [2.0]\n",
            "Epoch [40/70], Loss: 0.0970, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [41/70], Loss: 0.0931, Acc: 0.82 \n",
            "LR: [2.0]\n",
            "Epoch [42/70], Loss: 0.0903, Acc: 0.82 \n",
            "LR: [2.0]\n",
            "Epoch [43/70], Loss: 0.0959, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [44/70], Loss: 0.0879, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [45/70], Loss: 0.0907, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [46/70], Loss: 0.0880, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [47/70], Loss: 0.0862, Acc: 0.83 \n",
            "LR: [2.0]\n",
            "Epoch [48/70], Loss: 0.0937, Acc: 0.83 \n",
            "LR: [0.4]\n",
            "Epoch [49/70], Loss: 0.0900, Acc: 0.84 \n",
            "LR: [0.4]\n",
            "Epoch [50/70], Loss: 0.0932, Acc: 0.87 \n",
            "LR: [0.4]\n",
            "Epoch [51/70], Loss: 0.0898, Acc: 0.88 \n",
            "LR: [0.4]\n",
            "Epoch [52/70], Loss: 0.0860, Acc: 0.89 \n",
            "LR: [0.4]\n",
            "Epoch [53/70], Loss: 0.0935, Acc: 0.88 \n",
            "LR: [0.4]\n",
            "Epoch [54/70], Loss: 0.0857, Acc: 0.88 \n",
            "LR: [0.4]\n",
            "Epoch [55/70], Loss: 0.0926, Acc: 0.88 \n",
            "LR: [0.4]\n",
            "Epoch [56/70], Loss: 0.0891, Acc: 0.89 \n",
            "LR: [0.4]\n",
            "Epoch [57/70], Loss: 0.0905, Acc: 0.89 \n",
            "LR: [0.4]\n",
            "Epoch [58/70], Loss: 0.0914, Acc: 0.89 \n",
            "LR: [0.4]\n",
            "Epoch [59/70], Loss: 0.0908, Acc: 0.89 \n",
            "LR: [0.4]\n",
            "Epoch [60/70], Loss: 0.0916, Acc: 0.88 \n",
            "LR: [0.4]\n",
            "Epoch [61/70], Loss: 0.0818, Acc: 0.89 \n",
            "LR: [0.4]\n",
            "Epoch [62/70], Loss: 0.0910, Acc: 0.89 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [63/70], Loss: 0.0907, Acc: 0.89 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [64/70], Loss: 0.0908, Acc: 0.89 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [65/70], Loss: 0.0899, Acc: 0.89 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [66/70], Loss: 0.0896, Acc: 0.89 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [67/70], Loss: 0.0885, Acc: 0.89 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [68/70], Loss: 0.0922, Acc: 0.89 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [69/70], Loss: 0.0930, Acc: 0.89 \n",
            "LR: [0.08000000000000002]\n",
            "Epoch [70/70], Loss: 0.0859, Acc: 0.89 \n",
            "Constructing exemplar set for class-90...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:172: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Done\n",
            "Constructing exemplar set for class-91...\n",
            "Done\n",
            "Constructing exemplar set for class-92...\n",
            "Done\n",
            "Constructing exemplar set for class-93...\n",
            "Done\n",
            "Constructing exemplar set for class-94...\n",
            "Done\n",
            "Constructing exemplar set for class-95...\n",
            "Done\n",
            "Constructing exemplar set for class-96...\n",
            "Done\n",
            "Constructing exemplar set for class-97...\n",
            "Done\n",
            "Constructing exemplar set for class-98...\n",
            "Done\n",
            "Constructing exemplar set for class-99...\n",
            "Done\n",
            "iCaRL classes: 100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:62: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.387700 %\n",
            "0.3712\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZLxcnZsLYNg",
        "colab_type": "text"
      },
      "source": [
        "**Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJ7B5LYvbju5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 962
        },
        "outputId": "509151e8-27c6-41a1-c955-bcd06f723166"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(len(matrice))\n",
        "\n",
        "x = matrice[9]\n",
        "l = lab[9]\n",
        "\n",
        "l = [int(i) for i in l]\n",
        "x = [int(i) for i in x]\n",
        "\n",
        "cf = confusion_matrix(x,l)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15,13))\n",
        "im = ax.imshow(cf,cmap='plasma')\n",
        "\n",
        "ax.set_yticks([10,20,30,40,50,60,70,80,90])\n",
        "ax.set_xticks([10,20,30,40,50,60,70,80,90])\n",
        "\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "         rotation_mode=\"anchor\")\n",
        "\n",
        "ax.set_title(\"image classification\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x936 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AJmf1CuLdWD",
        "colab_type": "text"
      },
      "source": [
        "**Accuracy trend plot**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJEHzlQ2ogfu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bc048e8-c407-4994-e393-adcf672c8052"
      },
      "source": [
        "fig,ax = plt.subplots(figsize=(15,10))\n",
        "ax.plot([10,20,30,40,50,60,70,80,90,100],best_acc,'k-o')\n",
        "plt.xlim(10,100)\n",
        "plt.ylim(0,1)\n",
        "plt.xlabel('Classes')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n",
        "\n",
        "print(best_acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[0.792, 0.753, 0.6563333333333333, 0.61, 0.5536, 0.5156666666666667, 0.47314285714285714, 0.43575, 0.413, 0.3877]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9RUmO_zLhZK",
        "colab_type": "text"
      },
      "source": [
        "**Storing results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0SzqcjAp0ze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results_file = open(\"Results.txt\",\"a\")\n",
        "\n",
        "results_file.write(\"accuracy:\")\n",
        "results_file.write(str(best_acc))\n",
        "\n",
        "np.savetxt(\"CF_ICARL.txt\",cf,delimiter = ',')\n",
        "\n",
        "results_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}